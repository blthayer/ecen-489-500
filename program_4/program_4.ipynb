{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Delete data directory if present.\n",
    "shutil.rmtree('ECEN489Py4Data', ignore_errors=True)\n",
    "\n",
    "# Extract data.\n",
    "with zipfile.ZipFile(\"ECEN489Py4Data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall('ECEN489Py4Data')\n",
    "\n",
    "# Set directory for data.\n",
    "data_dir = os.path.join('ECEN489Py4Data', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataGenerator implements functions useful for input image scaling and augmentation -- you may want more!\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(data_dir, 'train'),\n",
    "        target_size=(32, 32),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=1953)\n",
    "\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        os.path.join(data_dir, 'validation'),\n",
    "        target_size=(32, 32),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False, #don't shuffle or label results will be all wrong\n",
    "        seed=1953)\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    os.path.join(data_dir, 'test'),\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False, #don't shuffle or label results will be all wrong\n",
    "    seed=1953)\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Presenting Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_results(m, g, steps):\n",
    "    Y_pred = m.predict_generator(g, steps=steps)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(g.classes, y_pred))\n",
    "    print('Classification Report')\n",
    "    print(classification_report(g.classes, y_pred, target_names=g.class_indices))\n",
    "    print(\"F1 score (using average='micro')\")\n",
    "    print(f1_score(y_true=g.classes, y_pred=y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Nowka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network model\n",
    "# you may want to vary these parameters, etc\n",
    "\n",
    "num_classes = 8 # fixed by the number of classes of signs that we gave you. Dont change\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Dense(num_classes))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# model.compile(loss = 'categorical_crossentropy',\n",
    "#               optimizer = 'Adam', # may want to try others\n",
    "#               metrics = ['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # Train and save.\n",
    "# model.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=10 # may need to increase if not seeing low enough losses\n",
    "# )\n",
    "# model.save('cnn_nowka.h5')\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Confusion Matrix\n",
      "[[ 52   0   0   0   0   0   0   0]\n",
      " [  0  49   1   0   0   0   0   0]\n",
      " [  0   1 195   2   0   0   2   0]\n",
      " [  0   0   2 148   0   0   4   0]\n",
      " [  0   0   0   0  17   0   0   0]\n",
      " [  0   0   0   0   0  47   0   0]\n",
      " [  1   0   2   1   0   0 383   0]\n",
      " [  0   0   0   0   0   1   3  18]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.98      1.00      0.99        52\n",
      "  pedestrian       0.98      0.98      0.98        50\n",
      "       yield       0.97      0.97      0.97       200\n",
      "        stop       0.98      0.96      0.97       154\n",
      "   keepRight       1.00      1.00      1.00        17\n",
      " signalAhead       0.98      1.00      0.99        47\n",
      "speedLimit25       0.98      0.99      0.98       387\n",
      "speedLimit35       1.00      0.82      0.90        22\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       929\n",
      "   macro avg       0.98      0.97      0.97       929\n",
      "weighted avg       0.98      0.98      0.98       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9784714747039828\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Model has been saved, loading it and making predictions.\n",
    "present_results(m=keras.models.load_model('cnn_nowka.h5'),\n",
    "                g=valid_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE CELLS BELOW TO COMPLETE THE EXERCISE WITH THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "# present_results(m=model, g=test_generator, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell dumps out a file of which files were incorrectly predicted\n",
    "#so you can see if you need more features, more training samples, etc\n",
    "# predicted_class_indices=np.argmax(Y_pred,axis=1)\n",
    "# labels = (train_generator.class_indices)\n",
    "# labels = dict((v,k) for k,v in labels.items())\n",
    "# predictions = [labels[k] for k in predicted_class_indices]\n",
    "# # NOTE/TODO: Change this to test_generator later.\n",
    "# filenames=valid_generator.filenames\n",
    "# # filenames=test_generator.filenames\n",
    "# print(len(filenames))\n",
    "# print(len(predictions))\n",
    "# results=pd.DataFrame({\"Filename\":filenames,\n",
    "#                       \"Predictions\":predictions})\n",
    "# results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete bad data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are in 'yield' but are actually 'merge'\n",
    "bad = [\n",
    "    '140_yield_1323813350.avi_image2.png',\n",
    "    '141_yield_1323813350.avi_image3.png',\n",
    "    '142_yield_1323813350.avi_image4.png',\n",
    "    '143_yield_1323813350.avi_image5.png',\n",
    "    '144_yield_1323813350.avi_image6.png',\n",
    "    '146_yield_1323813350.avi_image8.png',\n",
    "    '192_yield_1323816786.avi_image1.png',\n",
    "    '193_yield_1323816786.avi_image10.png',\n",
    "    '194_yield_1323816786.avi_image11.png',\n",
    "    '195_yield_1323816786.avi_image12.png',\n",
    "    '196_yield_1323816786.avi_image13.png',\n",
    "    '199_yield_1323816786.avi_image16.png',\n",
    "    '204_yield_1323816786.avi_image20.png',\n",
    "    '206_yield_1323816786.avi_image22.png',\n",
    "    '207_yield_1323816786.avi_image23.png',\n",
    "    '208_yield_1323816786.avi_image24.png',\n",
    "    '209_yield_1323816786.avi_image25.png',\n",
    "    '212_yield_1323816786.avi_image5.png',\n",
    "    '213_yield_1323816786.avi_image6.png',\n",
    "    '214_yield_1323816786.avi_image7.png',\n",
    "    '215_yield_1323816786.avi_image8.png',\n",
    "    '257_yield_1323821570.avi_image0.png',\n",
    "    '258_yield_1323821570.avi_image1.png',\n",
    "    '261_yield_1323821570.avi_image4.png',\n",
    "    '262_yield_1323821570.avi_image5.png',\n",
    "    '264_yield_1323821570.avi_image7.png',\n",
    "    '265_yield_1323821570.avi_image8.png',\n",
    "    \n",
    "]\n",
    "\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "for f in bad:\n",
    "    os.rename(os.path.join(train_dir, 'yield', f), os.path.join(train_dir, 'merge', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Image Data Generator(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2744 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use some rotations, shears, and flips \n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, shear_range=30,\n",
    "                                   horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(data_dir, 'train'),\n",
    "        target_size=(32, 32),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        seed=1953)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10,\n",
    "                                          restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 1 (deeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use more filters and one extra convolutional layer.\n",
    "# cnn = Sequential()\n",
    "# # Conv 1.\n",
    "# cnn.add(Conv2D(64, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Conv 2.\n",
    "# cnn.add(Conv2D(128, (3, 3), padding='same'))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Conv 3.\n",
    "# cnn.add(Conv2D(256, (2, 2), padding='same'))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Flatten.\n",
    "# cnn.add(Flatten())\n",
    "# # Dense.\n",
    "# cnn.add(Dense(512))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(Dropout(0.25))\n",
    "# # Predict.\n",
    "# cnn.add(Dense(num_classes))\n",
    "# cnn.add(Activation('softmax'))\n",
    "\n",
    "# # Print summary, compile.\n",
    "# cnn.summary()\n",
    "# cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train.\n",
    "# cnn.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=100,\n",
    "#                     callbacks=[early_stop]\n",
    "# )\n",
    "\n",
    "# # Save.\n",
    "# cnn.save('cnn_deeper.h5')\n",
    "\n",
    "# # Clear.\n",
    "# del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 2 (shallower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use larger initial convolution layer with more filters.\n",
    "# cnn = Sequential()\n",
    "# # Conv 1.\n",
    "# cnn.add(Conv2D(128, (4, 4), padding='same',input_shape = (32, 32, 3)))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Conv 2.\n",
    "# cnn.add(Conv2D(256, (2, 2), padding='same'))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# # Flatten.\n",
    "# cnn.add(Flatten())\n",
    "# # Dense.\n",
    "# cnn.add(Dense(512))\n",
    "# cnn.add(Activation('relu'))\n",
    "# cnn.add(Dropout(0.25))\n",
    "# # Predict.\n",
    "# cnn.add(Dense(num_classes))\n",
    "# cnn.add(Activation('softmax'))\n",
    "\n",
    "# # Print summary, compile.\n",
    "# cnn.summary()\n",
    "# cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train.\n",
    "# cnn.fit_generator(generator=train_generator,\n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     validation_steps=STEP_SIZE_VALID,\n",
    "#                     epochs=100,\n",
    "#                     callbacks=[early_stop]\n",
    "# )\n",
    "\n",
    "# # Save.\n",
    "# cnn.save('cnn_shallow.h5')\n",
    "\n",
    "# # Clear.\n",
    "# del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper:\n",
      "Confusion Matrix\n",
      "[[ 39   7   3   1   0   1   0   1]\n",
      " [  0  37  12   1   0   0   0   0]\n",
      " [  0   2 187  11   0   0   0   0]\n",
      " [  0   0   3 137   6   6   2   0]\n",
      " [  0   0   0   3   3  11   0   0]\n",
      " [  0   0   0   1   1  34  11   0]\n",
      " [  0   0   4   4   0   1 367  11]\n",
      " [ 11   0   0   0   0   0   4   7]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.78      0.75      0.76        52\n",
      "  pedestrian       0.80      0.74      0.77        50\n",
      "       yield       0.89      0.94      0.91       200\n",
      "        stop       0.87      0.89      0.88       154\n",
      "   keepRight       0.30      0.18      0.22        17\n",
      " signalAhead       0.64      0.72      0.68        47\n",
      "speedLimit25       0.96      0.95      0.95       387\n",
      "speedLimit35       0.37      0.32      0.34        22\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       929\n",
      "   macro avg       0.70      0.69      0.69       929\n",
      "weighted avg       0.87      0.87      0.87       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.8729817007534983\n",
      "********************************************************************************\n",
      "Shallower:\n",
      "Confusion Matrix\n",
      "[[ 30  19   3   0   0   0   0   0]\n",
      " [  0  28  22   0   0   0   0   0]\n",
      " [  0   0 177  23   0   0   0   0]\n",
      " [  0   0   1 130  13   9   1   0]\n",
      " [  0   0   0   0   0  17   0   0]\n",
      " [  0   0   0   1   0  25  21   0]\n",
      " [  0   0   3   1   0   2 361  20]\n",
      " [ 22   0   0   0   0   0   0   0]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.58      0.58      0.58        52\n",
      "  pedestrian       0.60      0.56      0.58        50\n",
      "       yield       0.86      0.89      0.87       200\n",
      "        stop       0.84      0.84      0.84       154\n",
      "   keepRight       0.00      0.00      0.00        17\n",
      " signalAhead       0.47      0.53      0.50        47\n",
      "speedLimit25       0.94      0.93      0.94       387\n",
      "speedLimit35       0.00      0.00      0.00        22\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       929\n",
      "   macro avg       0.54      0.54      0.54       929\n",
      "weighted avg       0.80      0.81      0.81       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.8083961248654468\n"
     ]
    }
   ],
   "source": [
    "# NOTE: models have been saved. Loading them up and making predictions.\n",
    "print('Deeper:')\n",
    "present_results(m=keras.models.load_model('cnn_deeper.h5'),\n",
    "                g=valid_generator, steps=STEP_SIZE_VALID)\n",
    "print('*'*80)\n",
    "print('Shallower:')\n",
    "present_results(m=keras.models.load_model('cnn_shallow.h5'),\n",
    "                g=valid_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present_results(m=cnn, g=test_generator, steps=STEP_SIZE_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
