{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Delete data directory if present.\n",
    "shutil.rmtree('ECEN489Py4Data', ignore_errors=True)\n",
    "\n",
    "# Extract data.\n",
    "with zipfile.ZipFile(\"ECEN489Py4Data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall('ECEN489Py4Data')\n",
    "\n",
    "with zipfile.ZipFile(\"ECEN489Py4TestData.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(\"ECEN489Py4Data/data\"))\n",
    "    \n",
    "# Set directory for data.\n",
    "DATA_DIR = os.path.join('ECEN489Py4Data', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataGenerator implements functions useful for input image scaling and augmentation -- you may want more!\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gen(image_dir, dg):\n",
    "    \"\"\"Initialize a flow_from_directory generator.\n",
    "    \n",
    "    We have to do this EACH TIME because otherwise we\n",
    "    get different results do to some sort of shifting\n",
    "    issue. \n",
    "    \"\"\"\n",
    "    if image_dir == 'train':\n",
    "        shuffle = True\n",
    "    elif image_dir == 'validation' or image_dir == 'test':\n",
    "        shuffle = False\n",
    "    else:\n",
    "        raise UserWarning('Typo!')\n",
    "        \n",
    "    g = dg.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, image_dir),\n",
    "        target_size=(32, 32),\n",
    "        color_mode='rgb',\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=shuffle,\n",
    "        seed=1953)\n",
    "    \n",
    "    return g, g.n//g.batch_size\n",
    "\n",
    "def present_results(m, image_dir, dg):\n",
    "    \"\"\"Given a model and information for image generation,\n",
    "    make predictions and present the information.\"\"\"\n",
    "    g, steps = init_gen(image_dir, dg)\n",
    "    Y_pred2 = m.predict_generator(g, steps=steps)\n",
    "    y_pred2 = np.argmax(Y_pred2, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(g.classes, y_pred2))\n",
    "    print('Classification Report')\n",
    "    print(classification_report(g.classes, y_pred2, target_names=g.class_indices))\n",
    "    print(\"F1 score (using average='micro')\")\n",
    "    print(f1_score(y_true=g.classes, y_pred=y_pred2, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN From Nowka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 2,121,160\n",
      "Trainable params: 2,121,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4216 - acc: 0.8732 - val_loss: 0.1542 - val_acc: 0.9591\n",
      "Epoch 2/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.1359 - acc: 0.9636 - val_loss: 0.1797 - val_acc: 0.9440\n",
      "Epoch 3/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0883 - acc: 0.9708 - val_loss: 0.1662 - val_acc: 0.9612\n",
      "Epoch 4/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0694 - acc: 0.9785 - val_loss: 0.1158 - val_acc: 0.9688\n",
      "Epoch 5/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0557 - acc: 0.9829 - val_loss: 0.1122 - val_acc: 0.9817\n",
      "Epoch 6/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0423 - acc: 0.9891 - val_loss: 0.1590 - val_acc: 0.9720\n",
      "Epoch 7/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0594 - acc: 0.9854 - val_loss: 0.2769 - val_acc: 0.9462\n",
      "Epoch 8/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0594 - acc: 0.9880 - val_loss: 0.1427 - val_acc: 0.9742\n",
      "Epoch 9/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0405 - acc: 0.9894 - val_loss: 0.2024 - val_acc: 0.9752\n",
      "Epoch 10/10\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.0618 - acc: 0.9883 - val_loss: 0.1233 - val_acc: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96d727c0b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural network model\n",
    "# you may want to vary these parameters, etc\n",
    "\n",
    "num_classes = 8 # fixed by the number of classes of signs that we gave you. Dont change\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'Adam', # may want to try others\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train and save.\n",
    "train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "model.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=10 # may need to increase if not seeing low enough losses\n",
    ")\n",
    "# model.save('cnn_nowka.h5')\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 929 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 52   0   0   0   0   0   0   0]\n",
      " [  0  49   1   0   0   0   0   0]\n",
      " [  0   1 195   2   0   0   2   0]\n",
      " [  0   0   2 148   0   0   4   0]\n",
      " [  0   0   0   0  17   0   0   0]\n",
      " [  0   0   0   0   0  47   0   0]\n",
      " [  1   0   2   1   0   0 383   0]\n",
      " [  0   0   0   0   0   1   3  18]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.98      1.00      0.99        52\n",
      "   keepRight       0.98      0.98      0.98        50\n",
      "       yield       0.97      0.97      0.97       200\n",
      "speedLimit35       0.98      0.96      0.97       154\n",
      "speedLimit25       1.00      1.00      1.00        17\n",
      " signalAhead       0.98      1.00      0.99        47\n",
      "  pedestrian       0.98      0.99      0.98       387\n",
      "        stop       1.00      0.82      0.90        22\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       929\n",
      "   macro avg       0.98      0.97      0.97       929\n",
      "weighted avg       0.98      0.98      0.98       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9784714747039828\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Model has been saved, loading it and making predictions.\n",
    "present_results(m=keras.models.load_model('cnn_nowka.h5'),\n",
    "                image_dir='validation', dg=valid_datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  53   1   0   0   0   1   0]\n",
      " [  0   0 204   4   0   0   0   0]\n",
      " [  0   1   2 140   0   0   3   0]\n",
      " [  0   0   0   0  23   1   1   0]\n",
      " [  0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   2   0   0 329   0]\n",
      " [  0   0   0   1   0   0   1  18]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        58\n",
      "   keepRight       0.98      0.96      0.97        55\n",
      "       yield       0.99      0.98      0.98       208\n",
      "speedLimit35       0.95      0.96      0.96       146\n",
      "speedLimit25       1.00      0.92      0.96        25\n",
      " signalAhead       0.98      1.00      0.99        41\n",
      "  pedestrian       0.98      0.99      0.99       331\n",
      "        stop       1.00      0.90      0.95        20\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       884\n",
      "   macro avg       0.98      0.96      0.97       884\n",
      "weighted avg       0.98      0.98      0.98       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9796380090497737\n"
     ]
    }
   ],
   "source": [
    "present_results(m=keras.models.load_model('cnn_nowka.h5'),\n",
    "                image_dir='test', dg=test_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell dumps out a file of which files were incorrectly predicted\n",
    "#so you can see if you need more features, more training samples, etc\n",
    "# predicted_class_indices=np.argmax(Y_pred,axis=1)\n",
    "# labels = (train_generator.class_indices)\n",
    "# labels = dict((v,k) for k,v in labels.items())\n",
    "# predictions = [labels[k] for k in predicted_class_indices]\n",
    "# # NOTE/TODO: Change this to test_generator later.\n",
    "# filenames=valid_generator.filenames\n",
    "# # filenames=test_generator.filenames\n",
    "# print(len(filenames))\n",
    "# print(len(predictions))\n",
    "# results=pd.DataFrame({\"Filename\":filenames,\n",
    "#                       \"Predictions\":predictions})\n",
    "# results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete bad data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are in 'yield' but are actually 'merge'\n",
    "bad = [\n",
    "    '140_yield_1323813350.avi_image2.png',\n",
    "    '141_yield_1323813350.avi_image3.png',\n",
    "    '142_yield_1323813350.avi_image4.png',\n",
    "    '143_yield_1323813350.avi_image5.png',\n",
    "    '144_yield_1323813350.avi_image6.png',\n",
    "    '146_yield_1323813350.avi_image8.png',\n",
    "    '192_yield_1323816786.avi_image1.png',\n",
    "    '193_yield_1323816786.avi_image10.png',\n",
    "    '194_yield_1323816786.avi_image11.png',\n",
    "    '195_yield_1323816786.avi_image12.png',\n",
    "    '196_yield_1323816786.avi_image13.png',\n",
    "    '199_yield_1323816786.avi_image16.png',\n",
    "    '204_yield_1323816786.avi_image20.png',\n",
    "    '206_yield_1323816786.avi_image22.png',\n",
    "    '207_yield_1323816786.avi_image23.png',\n",
    "    '208_yield_1323816786.avi_image24.png',\n",
    "    '209_yield_1323816786.avi_image25.png',\n",
    "    '212_yield_1323816786.avi_image5.png',\n",
    "    '213_yield_1323816786.avi_image6.png',\n",
    "    '214_yield_1323816786.avi_image7.png',\n",
    "    '215_yield_1323816786.avi_image8.png',\n",
    "    '257_yield_1323821570.avi_image0.png',\n",
    "    '258_yield_1323821570.avi_image1.png',\n",
    "    '260_yield_1323821570.avi_image3.png',\n",
    "    '261_yield_1323821570.avi_image4.png',\n",
    "    '262_yield_1323821570.avi_image5.png',\n",
    "    '264_yield_1323821570.avi_image7.png',\n",
    "    '265_yield_1323821570.avi_image8.png',\n",
    "]\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "for f in bad:\n",
    "    os.rename(os.path.join(train_dir, 'yield', f), os.path.join(train_dir, 'merge', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Image Data Generator(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some rotations, shears, shifts.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, shear_range=30,\n",
    "                                   width_shift_range=0.25, height_shift_range=0.25, zoom_range=0.25)\n",
    "                                   #horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "# train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "# val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "# Our dataset is heavily imbalanced. Use class weights.\n",
    "# NOTE: It turns out using weights really impacts training negatively.\n",
    "# classes = train_gen.classes\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(classes), y=classes)\n",
    "# weight_dict = {ind: val for ind, val in enumerate(class_weights)}\n",
    "# print(weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20,\n",
    "                                          restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 1 (deeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         131328    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 2,308,744\n",
      "Trainable params: 2,308,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "Epoch 1/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 1.6580 - acc: 0.3929 - val_loss: 1.4314 - val_acc: 0.4532\n",
      "Epoch 2/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 1.2914 - acc: 0.5368 - val_loss: 0.8621 - val_acc: 0.6803\n",
      "Epoch 3/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 1.0469 - acc: 0.6217 - val_loss: 0.6848 - val_acc: 0.7750\n",
      "Epoch 4/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.9023 - acc: 0.6851 - val_loss: 0.5463 - val_acc: 0.7901\n",
      "Epoch 5/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.8397 - acc: 0.7063 - val_loss: 0.5400 - val_acc: 0.8041\n",
      "Epoch 6/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.7560 - acc: 0.7340 - val_loss: 0.3916 - val_acc: 0.8568\n",
      "Epoch 7/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.7425 - acc: 0.7467 - val_loss: 0.3904 - val_acc: 0.8719\n",
      "Epoch 8/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.6560 - acc: 0.7733 - val_loss: 0.4274 - val_acc: 0.8654\n",
      "Epoch 9/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.6425 - acc: 0.7879 - val_loss: 0.3475 - val_acc: 0.8698\n",
      "Epoch 10/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.6068 - acc: 0.7861 - val_loss: 0.3560 - val_acc: 0.8708\n",
      "Epoch 11/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.6186 - acc: 0.7872 - val_loss: 0.3120 - val_acc: 0.9031\n",
      "Epoch 12/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.5656 - acc: 0.7985 - val_loss: 0.3547 - val_acc: 0.8784\n",
      "Epoch 13/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.5452 - acc: 0.8043 - val_loss: 0.2706 - val_acc: 0.9107\n",
      "Epoch 14/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.5071 - acc: 0.8138 - val_loss: 0.3221 - val_acc: 0.8837\n",
      "Epoch 15/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.5202 - acc: 0.8083 - val_loss: 0.2948 - val_acc: 0.9031\n",
      "Epoch 16/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4961 - acc: 0.8262 - val_loss: 0.2362 - val_acc: 0.9182\n",
      "Epoch 17/100\n",
      "2744/2744 [==============================] - 21s 8ms/step - loss: 0.4803 - acc: 0.8265 - val_loss: 0.3190 - val_acc: 0.8848\n",
      "Epoch 18/100\n",
      "2744/2744 [==============================] - 21s 8ms/step - loss: 0.4940 - acc: 0.8335 - val_loss: 0.2386 - val_acc: 0.9279\n",
      "Epoch 19/100\n",
      "2744/2744 [==============================] - 21s 8ms/step - loss: 0.5302 - acc: 0.8294 - val_loss: 0.2705 - val_acc: 0.8999\n",
      "Epoch 20/100\n",
      "2744/2744 [==============================] - 21s 7ms/step - loss: 0.4318 - acc: 0.8466 - val_loss: 0.3751 - val_acc: 0.8924\n",
      "Epoch 21/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4646 - acc: 0.8455 - val_loss: 0.2517 - val_acc: 0.9203\n",
      "Epoch 22/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4504 - acc: 0.8466 - val_loss: 0.2629 - val_acc: 0.9096\n",
      "Epoch 23/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4285 - acc: 0.8542 - val_loss: 0.2228 - val_acc: 0.9343\n",
      "Epoch 24/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4531 - acc: 0.8473 - val_loss: 0.2187 - val_acc: 0.9268\n",
      "Epoch 25/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.5308 - acc: 0.8437 - val_loss: 0.2701 - val_acc: 0.9182\n",
      "Epoch 26/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4521 - acc: 0.8535 - val_loss: 0.3026 - val_acc: 0.9042\n",
      "Epoch 27/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4287 - acc: 0.8524 - val_loss: 0.2047 - val_acc: 0.9182\n",
      "Epoch 28/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4027 - acc: 0.8663 - val_loss: 0.2511 - val_acc: 0.9376\n",
      "Epoch 29/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4393 - acc: 0.8495 - val_loss: 0.2255 - val_acc: 0.9247\n",
      "Epoch 30/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4556 - acc: 0.8619 - val_loss: 0.2194 - val_acc: 0.9193\n",
      "Epoch 31/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4233 - acc: 0.8601 - val_loss: 0.2143 - val_acc: 0.9247\n",
      "Epoch 32/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4515 - acc: 0.8630 - val_loss: 0.2389 - val_acc: 0.9225\n",
      "Epoch 33/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3861 - acc: 0.8673 - val_loss: 0.2514 - val_acc: 0.9182\n",
      "Epoch 34/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3662 - acc: 0.8816 - val_loss: 0.2556 - val_acc: 0.9386\n",
      "Epoch 35/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3971 - acc: 0.8728 - val_loss: 0.4505 - val_acc: 0.8924\n",
      "Epoch 36/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4529 - acc: 0.8641 - val_loss: 0.1973 - val_acc: 0.9397\n",
      "Epoch 37/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4169 - acc: 0.8673 - val_loss: 0.2347 - val_acc: 0.9214\n",
      "Epoch 38/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3964 - acc: 0.8699 - val_loss: 0.2384 - val_acc: 0.9214\n",
      "Epoch 39/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4045 - acc: 0.8728 - val_loss: 0.2351 - val_acc: 0.9322\n",
      "Epoch 40/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4087 - acc: 0.8670 - val_loss: 0.1951 - val_acc: 0.9333\n",
      "Epoch 41/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4500 - acc: 0.8630 - val_loss: 0.2117 - val_acc: 0.9290\n",
      "Epoch 42/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4012 - acc: 0.8732 - val_loss: 0.2154 - val_acc: 0.9451\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4098 - acc: 0.8670 - val_loss: 0.1973 - val_acc: 0.9386\n",
      "Epoch 44/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4034 - acc: 0.8695 - val_loss: 0.2801 - val_acc: 0.9117\n",
      "Epoch 45/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4306 - acc: 0.8728 - val_loss: 0.3910 - val_acc: 0.9064\n",
      "Epoch 46/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3835 - acc: 0.8779 - val_loss: 0.2291 - val_acc: 0.9429\n",
      "Epoch 47/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4262 - acc: 0.8703 - val_loss: 0.2282 - val_acc: 0.9300\n",
      "Epoch 48/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4012 - acc: 0.8772 - val_loss: 0.1829 - val_acc: 0.9365\n",
      "Epoch 49/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4174 - acc: 0.8728 - val_loss: 0.3015 - val_acc: 0.9268\n",
      "Epoch 50/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3614 - acc: 0.8848 - val_loss: 0.2233 - val_acc: 0.9160\n",
      "Epoch 51/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3746 - acc: 0.8852 - val_loss: 0.2257 - val_acc: 0.9333\n",
      "Epoch 52/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3767 - acc: 0.8776 - val_loss: 0.1792 - val_acc: 0.9483\n",
      "Epoch 53/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3881 - acc: 0.8746 - val_loss: 0.1637 - val_acc: 0.9526\n",
      "Epoch 54/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3953 - acc: 0.8754 - val_loss: 0.1876 - val_acc: 0.9376\n",
      "Epoch 55/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4588 - acc: 0.8746 - val_loss: 0.1689 - val_acc: 0.9516\n",
      "Epoch 56/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3785 - acc: 0.8929 - val_loss: 0.2850 - val_acc: 0.8988\n",
      "Epoch 57/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4033 - acc: 0.8710 - val_loss: 0.3125 - val_acc: 0.9139\n",
      "Epoch 58/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3890 - acc: 0.8776 - val_loss: 0.2281 - val_acc: 0.9247\n",
      "Epoch 59/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3911 - acc: 0.8797 - val_loss: 0.3241 - val_acc: 0.8977\n",
      "Epoch 60/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4388 - acc: 0.8681 - val_loss: 0.2266 - val_acc: 0.9419\n",
      "Epoch 61/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3989 - acc: 0.8816 - val_loss: 0.1982 - val_acc: 0.9322\n",
      "Epoch 62/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3793 - acc: 0.8739 - val_loss: 0.2159 - val_acc: 0.9429\n",
      "Epoch 63/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4380 - acc: 0.8604 - val_loss: 0.2376 - val_acc: 0.9247\n",
      "Epoch 64/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4001 - acc: 0.8735 - val_loss: 0.2024 - val_acc: 0.9429\n",
      "Epoch 65/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3816 - acc: 0.8794 - val_loss: 0.2247 - val_acc: 0.9225\n",
      "Epoch 66/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3588 - acc: 0.8786 - val_loss: 0.1865 - val_acc: 0.9440\n",
      "Epoch 67/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3824 - acc: 0.8816 - val_loss: 0.1989 - val_acc: 0.9419\n",
      "Epoch 68/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4509 - acc: 0.8699 - val_loss: 0.2258 - val_acc: 0.9343\n",
      "Epoch 69/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3860 - acc: 0.8827 - val_loss: 0.2140 - val_acc: 0.9354\n",
      "Epoch 70/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3929 - acc: 0.8681 - val_loss: 0.2192 - val_acc: 0.9279\n",
      "Epoch 71/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.3373 - acc: 0.8878 - val_loss: 0.2730 - val_acc: 0.9247\n",
      "Epoch 72/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4067 - acc: 0.8808 - val_loss: 0.2950 - val_acc: 0.9042\n",
      "Epoch 73/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4362 - acc: 0.8666 - val_loss: 0.1642 - val_acc: 0.9494\n"
     ]
    }
   ],
   "source": [
    "deep_file = 'cnn_deeper.h5' \n",
    "\n",
    "# Use more filters and one extra convolutional layer.\n",
    "cnn = Sequential()\n",
    "# Conv 1.\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Conv 2.\n",
    "cnn.add(Conv2D(128, (3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Conv 3.\n",
    "cnn.add(Conv2D(256, (2, 2), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flatten.\n",
    "cnn.add(Flatten())\n",
    "# Dense.\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(0.25))\n",
    "# Predict.\n",
    "cnn.add(Dense(num_classes))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "# Print summary, compile.\n",
    "cnn.summary()\n",
    "cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Recreate generators to avoid annoying shifting issues.\n",
    "train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "# Train.\n",
    "cnn.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks=[early_stop]\n",
    "                    #class_weight=weight_dict\n",
    ")\n",
    "\n",
    "# Save.\n",
    "cnn.save(deep_file)\n",
    "\n",
    "# Clear.\n",
    "del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 2 (shallower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       6272      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       131328    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 8,530,824\n",
      "Trainable params: 8,530,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "Epoch 1/100\n",
      "2744/2744 [==============================] - 51s 18ms/step - loss: 1.5417 - acc: 0.4453 - val_loss: 1.0936 - val_acc: 0.6060\n",
      "Epoch 2/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 1.1734 - acc: 0.5780 - val_loss: 1.2064 - val_acc: 0.5931\n",
      "Epoch 3/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.9778 - acc: 0.6542 - val_loss: 0.8187 - val_acc: 0.7061\n",
      "Epoch 4/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.9269 - acc: 0.6742 - val_loss: 0.7263 - val_acc: 0.7363\n",
      "Epoch 5/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.8330 - acc: 0.7092 - val_loss: 0.4903 - val_acc: 0.8299\n",
      "Epoch 6/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.7595 - acc: 0.7325 - val_loss: 0.4694 - val_acc: 0.8310\n",
      "Epoch 7/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.7265 - acc: 0.7427 - val_loss: 0.4005 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.6921 - acc: 0.7584 - val_loss: 0.3879 - val_acc: 0.8579\n",
      "Epoch 9/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.6590 - acc: 0.7795 - val_loss: 0.4087 - val_acc: 0.8654\n",
      "Epoch 10/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.6334 - acc: 0.7835 - val_loss: 0.4381 - val_acc: 0.8288\n",
      "Epoch 11/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.6002 - acc: 0.7919 - val_loss: 0.3429 - val_acc: 0.8913\n",
      "Epoch 12/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5862 - acc: 0.7988 - val_loss: 0.3960 - val_acc: 0.8590\n",
      "Epoch 13/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5824 - acc: 0.7996 - val_loss: 0.3247 - val_acc: 0.8902\n",
      "Epoch 14/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5569 - acc: 0.8069 - val_loss: 0.3024 - val_acc: 0.8902\n",
      "Epoch 15/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5274 - acc: 0.8203 - val_loss: 0.2431 - val_acc: 0.9139\n",
      "Epoch 16/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5492 - acc: 0.8120 - val_loss: 0.4408 - val_acc: 0.8558\n",
      "Epoch 17/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4968 - acc: 0.8386 - val_loss: 0.2665 - val_acc: 0.9139\n",
      "Epoch 18/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.5312 - acc: 0.8229 - val_loss: 0.2494 - val_acc: 0.9268\n",
      "Epoch 19/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.5029 - acc: 0.8367 - val_loss: 0.2904 - val_acc: 0.9053\n",
      "Epoch 20/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4938 - acc: 0.8342 - val_loss: 0.3070 - val_acc: 0.8956\n",
      "Epoch 21/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4581 - acc: 0.8448 - val_loss: 0.2625 - val_acc: 0.9096\n",
      "Epoch 22/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4880 - acc: 0.8353 - val_loss: 0.2864 - val_acc: 0.9074\n",
      "Epoch 23/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4706 - acc: 0.8397 - val_loss: 0.2988 - val_acc: 0.8977\n",
      "Epoch 24/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4817 - acc: 0.8378 - val_loss: 0.2798 - val_acc: 0.9096\n",
      "Epoch 25/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4284 - acc: 0.8608 - val_loss: 0.2959 - val_acc: 0.9236\n",
      "Epoch 26/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4686 - acc: 0.8473 - val_loss: 0.2695 - val_acc: 0.9085\n",
      "Epoch 27/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4573 - acc: 0.8491 - val_loss: 0.2616 - val_acc: 0.9193\n",
      "Epoch 28/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4371 - acc: 0.8517 - val_loss: 0.2270 - val_acc: 0.9279\n",
      "Epoch 29/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4516 - acc: 0.8495 - val_loss: 0.3343 - val_acc: 0.9010\n",
      "Epoch 30/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4254 - acc: 0.8601 - val_loss: 0.2208 - val_acc: 0.9247\n",
      "Epoch 31/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4640 - acc: 0.8597 - val_loss: 0.2442 - val_acc: 0.9354\n",
      "Epoch 32/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4353 - acc: 0.8513 - val_loss: 0.3022 - val_acc: 0.8913\n",
      "Epoch 33/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4126 - acc: 0.8655 - val_loss: 0.2228 - val_acc: 0.9429\n",
      "Epoch 34/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4298 - acc: 0.8644 - val_loss: 0.2306 - val_acc: 0.9268\n",
      "Epoch 35/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.3913 - acc: 0.8717 - val_loss: 0.3248 - val_acc: 0.8956\n",
      "Epoch 36/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4276 - acc: 0.8626 - val_loss: 0.2533 - val_acc: 0.9203\n",
      "Epoch 37/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4340 - acc: 0.8659 - val_loss: 0.2675 - val_acc: 0.9182\n",
      "Epoch 38/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.3894 - acc: 0.8739 - val_loss: 0.3129 - val_acc: 0.9010\n",
      "Epoch 39/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4368 - acc: 0.8553 - val_loss: 0.2859 - val_acc: 0.9160\n",
      "Epoch 40/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4245 - acc: 0.8655 - val_loss: 0.2603 - val_acc: 0.9160\n",
      "Epoch 41/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4431 - acc: 0.8553 - val_loss: 0.2971 - val_acc: 0.9053\n",
      "Epoch 42/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.3789 - acc: 0.8790 - val_loss: 0.3198 - val_acc: 0.9171\n",
      "Epoch 43/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.3985 - acc: 0.8724 - val_loss: 0.3187 - val_acc: 0.9064\n",
      "Epoch 44/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4492 - acc: 0.8619 - val_loss: 0.2540 - val_acc: 0.9311\n",
      "Epoch 45/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4451 - acc: 0.8732 - val_loss: 0.4601 - val_acc: 0.8956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4304 - acc: 0.8692 - val_loss: 0.2495 - val_acc: 0.9107\n",
      "Epoch 47/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4198 - acc: 0.8743 - val_loss: 0.3235 - val_acc: 0.9074\n",
      "Epoch 48/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.4375 - acc: 0.8659 - val_loss: 0.2657 - val_acc: 0.9268\n",
      "Epoch 49/100\n",
      "2744/2744 [==============================] - 49s 18ms/step - loss: 0.3946 - acc: 0.8776 - val_loss: 0.2771 - val_acc: 0.9171\n",
      "Epoch 50/100\n",
      "2744/2744 [==============================] - 50s 18ms/step - loss: 0.4621 - acc: 0.8633 - val_loss: 0.2231 - val_acc: 0.9397\n"
     ]
    }
   ],
   "source": [
    "shallow_file = 'cnn_shallow.h5' \n",
    "\n",
    "# Use larger initial convolution layer with more filters.\n",
    "cnn = Sequential()\n",
    "# Conv 1.\n",
    "cnn.add(Conv2D(128, (4, 4), padding='same',input_shape = (32, 32, 3)))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Conv 2.\n",
    "cnn.add(Conv2D(256, (2, 2), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flatten.\n",
    "cnn.add(Flatten())\n",
    "# Dense.\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(0.25))\n",
    "# Predict.\n",
    "cnn.add(Dense(num_classes))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "# Print summary, compile.\n",
    "cnn.summary()\n",
    "cnn.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Recreate generators to avoid annoying shifting issues.\n",
    "train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "# Train.\n",
    "cnn.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks=[early_stop]\n",
    "                    #class_weight=weight_dict\n",
    ")\n",
    "\n",
    "# Save.\n",
    "cnn.save(shallow_file)\n",
    "\n",
    "# Clear.\n",
    "del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 2,121,160\n",
      "Trainable params: 2,121,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "Epoch 1/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 1.4339 - acc: 0.4770 - val_loss: 1.3788 - val_acc: 0.5823\n",
      "Epoch 2/100\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 1.0901 - acc: 0.5973 - val_loss: 0.9974 - val_acc: 0.6469\n",
      "Epoch 3/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.9566 - acc: 0.6589 - val_loss: 0.6442 - val_acc: 0.7438\n",
      "Epoch 4/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.8636 - acc: 0.6942 - val_loss: 0.5909 - val_acc: 0.7966\n",
      "Epoch 5/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.7709 - acc: 0.7274 - val_loss: 0.4732 - val_acc: 0.8256\n",
      "Epoch 6/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.7376 - acc: 0.7354 - val_loss: 0.4194 - val_acc: 0.8568\n",
      "Epoch 7/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.6860 - acc: 0.7547 - val_loss: 0.3988 - val_acc: 0.8536\n",
      "Epoch 8/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.6480 - acc: 0.7777 - val_loss: 0.3779 - val_acc: 0.8676\n",
      "Epoch 9/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.6100 - acc: 0.7901 - val_loss: 0.3418 - val_acc: 0.8837\n",
      "Epoch 10/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.6132 - acc: 0.7930 - val_loss: 0.4228 - val_acc: 0.8321\n",
      "Epoch 11/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5926 - acc: 0.7977 - val_loss: 0.4081 - val_acc: 0.8396\n",
      "Epoch 12/100\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.5777 - acc: 0.8036 - val_loss: 0.3605 - val_acc: 0.8741\n",
      "Epoch 13/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5422 - acc: 0.8105 - val_loss: 0.4178 - val_acc: 0.8558\n",
      "Epoch 14/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5487 - acc: 0.8109 - val_loss: 0.3972 - val_acc: 0.8676\n",
      "Epoch 15/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5278 - acc: 0.8178 - val_loss: 0.3596 - val_acc: 0.8816\n",
      "Epoch 16/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4945 - acc: 0.8269 - val_loss: 0.3095 - val_acc: 0.8967\n",
      "Epoch 17/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5174 - acc: 0.8214 - val_loss: 0.3024 - val_acc: 0.9031\n",
      "Epoch 18/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5135 - acc: 0.8313 - val_loss: 0.2502 - val_acc: 0.9117\n",
      "Epoch 19/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4815 - acc: 0.8371 - val_loss: 0.5154 - val_acc: 0.8558\n",
      "Epoch 20/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4912 - acc: 0.8342 - val_loss: 0.2872 - val_acc: 0.8999\n",
      "Epoch 21/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4801 - acc: 0.8360 - val_loss: 0.2635 - val_acc: 0.9160\n",
      "Epoch 22/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4598 - acc: 0.8437 - val_loss: 0.2901 - val_acc: 0.9128\n",
      "Epoch 23/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4703 - acc: 0.8378 - val_loss: 0.2708 - val_acc: 0.9074\n",
      "Epoch 24/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4720 - acc: 0.8473 - val_loss: 0.2770 - val_acc: 0.9085\n",
      "Epoch 25/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5083 - acc: 0.8393 - val_loss: 0.2169 - val_acc: 0.9236\n",
      "Epoch 26/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4495 - acc: 0.8520 - val_loss: 0.3119 - val_acc: 0.8913\n",
      "Epoch 27/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4660 - acc: 0.8444 - val_loss: 0.2291 - val_acc: 0.9236\n",
      "Epoch 28/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3935 - acc: 0.8641 - val_loss: 0.2744 - val_acc: 0.9311\n",
      "Epoch 29/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4851 - acc: 0.8499 - val_loss: 0.2925 - val_acc: 0.9064\n",
      "Epoch 30/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4298 - acc: 0.8615 - val_loss: 0.2335 - val_acc: 0.9085\n",
      "Epoch 31/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4271 - acc: 0.8546 - val_loss: 0.3154 - val_acc: 0.8977\n",
      "Epoch 32/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4511 - acc: 0.8520 - val_loss: 0.2625 - val_acc: 0.9171\n",
      "Epoch 33/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4198 - acc: 0.8502 - val_loss: 0.2450 - val_acc: 0.9171\n",
      "Epoch 34/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4179 - acc: 0.8644 - val_loss: 0.3293 - val_acc: 0.8977\n",
      "Epoch 35/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3938 - acc: 0.8677 - val_loss: 0.2332 - val_acc: 0.9193\n",
      "Epoch 36/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4228 - acc: 0.8590 - val_loss: 0.3184 - val_acc: 0.9214\n",
      "Epoch 37/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4476 - acc: 0.8517 - val_loss: 0.2148 - val_acc: 0.9268\n",
      "Epoch 38/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5684 - acc: 0.8353 - val_loss: 0.2901 - val_acc: 0.9085\n",
      "Epoch 39/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4276 - acc: 0.8593 - val_loss: 0.2828 - val_acc: 0.9247\n",
      "Epoch 40/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4352 - acc: 0.8582 - val_loss: 0.2571 - val_acc: 0.9311\n",
      "Epoch 41/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.5283 - acc: 0.8535 - val_loss: 0.3730 - val_acc: 0.8913\n",
      "Epoch 42/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4403 - acc: 0.8637 - val_loss: 0.1871 - val_acc: 0.9365\n",
      "Epoch 43/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4144 - acc: 0.8724 - val_loss: 0.3636 - val_acc: 0.8859\n",
      "Epoch 44/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4114 - acc: 0.8721 - val_loss: 0.2015 - val_acc: 0.9386\n",
      "Epoch 45/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4181 - acc: 0.8663 - val_loss: 0.3481 - val_acc: 0.9042\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4270 - acc: 0.8699 - val_loss: 0.2628 - val_acc: 0.9107\n",
      "Epoch 47/100\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.4262 - acc: 0.8597 - val_loss: 0.2178 - val_acc: 0.9247\n",
      "Epoch 48/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4293 - acc: 0.8655 - val_loss: 0.2128 - val_acc: 0.9300\n",
      "Epoch 49/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4273 - acc: 0.8692 - val_loss: 0.2221 - val_acc: 0.9257\n",
      "Epoch 50/100\n",
      "2744/2744 [==============================] - 18s 7ms/step - loss: 0.4106 - acc: 0.8637 - val_loss: 0.2322 - val_acc: 0.9386\n",
      "Epoch 51/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3880 - acc: 0.8728 - val_loss: 0.1740 - val_acc: 0.9473\n",
      "Epoch 52/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4048 - acc: 0.8677 - val_loss: 0.2375 - val_acc: 0.9300\n",
      "Epoch 53/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4553 - acc: 0.8630 - val_loss: 0.2917 - val_acc: 0.8988\n",
      "Epoch 54/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3889 - acc: 0.8768 - val_loss: 0.2161 - val_acc: 0.9451\n",
      "Epoch 55/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3821 - acc: 0.8808 - val_loss: 0.2284 - val_acc: 0.9473\n",
      "Epoch 56/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4241 - acc: 0.8721 - val_loss: 0.2651 - val_acc: 0.9203\n",
      "Epoch 57/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3723 - acc: 0.8797 - val_loss: 0.2128 - val_acc: 0.9462\n",
      "Epoch 58/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4247 - acc: 0.8666 - val_loss: 0.2149 - val_acc: 0.9354\n",
      "Epoch 59/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4118 - acc: 0.8633 - val_loss: 0.2051 - val_acc: 0.9376\n",
      "Epoch 60/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4174 - acc: 0.8754 - val_loss: 0.2805 - val_acc: 0.9171\n",
      "Epoch 61/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3908 - acc: 0.8750 - val_loss: 0.2514 - val_acc: 0.9225\n",
      "Epoch 62/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3905 - acc: 0.8739 - val_loss: 0.2045 - val_acc: 0.9386\n",
      "Epoch 63/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4040 - acc: 0.8706 - val_loss: 0.2087 - val_acc: 0.9408\n",
      "Epoch 64/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4141 - acc: 0.8710 - val_loss: 0.2146 - val_acc: 0.9408\n",
      "Epoch 65/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3791 - acc: 0.8816 - val_loss: 0.2317 - val_acc: 0.9311\n",
      "Epoch 66/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.3495 - acc: 0.8874 - val_loss: 0.2128 - val_acc: 0.9386\n",
      "Epoch 67/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4024 - acc: 0.8827 - val_loss: 0.2195 - val_acc: 0.9247\n",
      "Epoch 68/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4093 - acc: 0.8772 - val_loss: 0.2325 - val_acc: 0.9408\n",
      "Epoch 69/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4134 - acc: 0.8841 - val_loss: 0.3332 - val_acc: 0.9074\n",
      "Epoch 70/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4199 - acc: 0.8801 - val_loss: 0.2440 - val_acc: 0.9386\n",
      "Epoch 71/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.4120 - acc: 0.8779 - val_loss: 0.2123 - val_acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "nowka_file = 'cnn_nowka_mod.h5'\n",
    "\n",
    "# Use Nowka's CNN, but use early stopping. Don't use class weights.\n",
    "# Also note that we'll be using the new train_generator, which performs\n",
    "# some image augmentation, and has the bad images moved.\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Dense(num_classes))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "cnn.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'Adam',\n",
    "              metrics = ['accuracy'])\n",
    "cnn.summary()\n",
    "\n",
    "# Recreate generators to avoid annoying shifting issues.\n",
    "train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "# Train.\n",
    "cnn.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks=[early_stop]\n",
    "                    #class_weight=weight_dict\n",
    ")\n",
    "\n",
    "# Save.\n",
    "cnn.save(nowka_file)\n",
    "\n",
    "# Clear.\n",
    "del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 2,121,160\n",
      "Trainable params: 2,121,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 2744 images belonging to 8 classes.\n",
      "Found 929 images belonging to 8 classes.\n",
      "Epoch 1/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.4523 - acc: 0.8590 - val_loss: 0.0788 - val_acc: 0.9839\n",
      "Epoch 2/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0998 - acc: 0.9716 - val_loss: 0.1383 - val_acc: 0.9709\n",
      "Epoch 3/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0752 - acc: 0.9778 - val_loss: 0.1034 - val_acc: 0.9795\n",
      "Epoch 4/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0462 - acc: 0.9872 - val_loss: 0.2479 - val_acc: 0.9365\n",
      "Epoch 5/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0516 - acc: 0.9851 - val_loss: 0.1320 - val_acc: 0.9785\n",
      "Epoch 6/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0560 - acc: 0.9869 - val_loss: 0.0981 - val_acc: 0.9828\n",
      "Epoch 7/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0442 - acc: 0.9894 - val_loss: 0.1385 - val_acc: 0.9677\n",
      "Epoch 8/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0344 - acc: 0.9920 - val_loss: 0.1326 - val_acc: 0.9806\n",
      "Epoch 9/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0602 - acc: 0.9858 - val_loss: 0.1292 - val_acc: 0.9795\n",
      "Epoch 10/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0393 - acc: 0.9923 - val_loss: 0.1998 - val_acc: 0.9677\n",
      "Epoch 11/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0359 - acc: 0.9927 - val_loss: 0.1251 - val_acc: 0.9828\n",
      "Epoch 12/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0561 - acc: 0.9883 - val_loss: 0.1383 - val_acc: 0.9785\n",
      "Epoch 13/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0573 - acc: 0.9891 - val_loss: 0.1803 - val_acc: 0.9795\n",
      "Epoch 14/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0683 - acc: 0.9913 - val_loss: 0.3875 - val_acc: 0.9569\n",
      "Epoch 15/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0538 - acc: 0.9920 - val_loss: 0.1396 - val_acc: 0.9828\n",
      "Epoch 16/100\n",
      "2744/2744 [==============================] - 19s 7ms/step - loss: 0.0951 - acc: 0.9894 - val_loss: 0.2274 - val_acc: 0.9731\n",
      "Epoch 17/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0244 - acc: 0.9964 - val_loss: 0.1746 - val_acc: 0.9795\n",
      "Epoch 18/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0891 - acc: 0.9872 - val_loss: 0.1597 - val_acc: 0.9785\n",
      "Epoch 19/100\n",
      "2744/2744 [==============================] - 21s 8ms/step - loss: 0.0161 - acc: 0.9964 - val_loss: 0.1561 - val_acc: 0.9817\n",
      "Epoch 20/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0904 - acc: 0.9902 - val_loss: 0.1597 - val_acc: 0.9828\n",
      "Epoch 21/100\n",
      "2744/2744 [==============================] - 20s 7ms/step - loss: 0.0322 - acc: 0.9953 - val_loss: 0.1771 - val_acc: 0.9839\n"
     ]
    }
   ],
   "source": [
    "no_aug = 'cnn_no_aug.h5'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# Use Nowka's CNN, but with early stopping. Don't bother with image augmentation,\n",
    "# as that seems to be leading to not as good results. This means that the data\n",
    "# is likely pretty homogenous (which is true from looking at it), so the \n",
    "# robustness that comes from augmentation actually hurts us here.\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, (3, 3), padding='same',input_shape = (32, 32, 3)))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(512))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Dense(num_classes))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "cnn.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'Adam',\n",
    "              metrics = ['accuracy'])\n",
    "cnn.summary()\n",
    "\n",
    "# Recreate generators to avoid annoying shifting issues.\n",
    "train_gen, train_steps = init_gen(image_dir='train', dg=train_datagen)\n",
    "val_gen, val_steps = init_gen(image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "# Train.\n",
    "cnn.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=100,\n",
    "                    callbacks=[early_stop]\n",
    "                    #class_weight=weight_dict\n",
    ")\n",
    "\n",
    "# Save.\n",
    "cnn.save(no_aug)\n",
    "\n",
    "# Clear.\n",
    "del cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper:\n",
      "Found 929 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 51   0   0   0   0   1   0   0]\n",
      " [  0  43   3   3   0   0   1   0]\n",
      " [  2   1 193   4   0   0   0   0]\n",
      " [  0   0   3 150   0   0   1   0]\n",
      " [  0   0   0   0  14   3   0   0]\n",
      " [  0   0   0   0   7  40   0   0]\n",
      " [  0   1   3   2   0   0 380   1]\n",
      " [  0   0   4   0   0   0   4  14]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.96      0.98      0.97        52\n",
      "   keepRight       0.96      0.86      0.91        50\n",
      "       yield       0.94      0.96      0.95       200\n",
      "speedLimit35       0.94      0.97      0.96       154\n",
      "speedLimit25       0.67      0.82      0.74        17\n",
      " signalAhead       0.91      0.85      0.88        47\n",
      "  pedestrian       0.98      0.98      0.98       387\n",
      "        stop       0.93      0.64      0.76        22\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       929\n",
      "   macro avg       0.91      0.88      0.89       929\n",
      "weighted avg       0.95      0.95      0.95       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9526372443487621\n",
      "********************************************************************************\n",
      "Shallower:\n",
      "Found 929 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 51   0   0   0   0   0   1   0]\n",
      " [  0  38   3   8   0   0   1   0]\n",
      " [  1   3 189   7   0   0   0   0]\n",
      " [  1   2   7 141   0   0   3   0]\n",
      " [  0   0   0   0  11   6   0   0]\n",
      " [  0   0   0   0  11  35   1   0]\n",
      " [  0   0   2   0   0   1 384   0]\n",
      " [  0   0   1   0   0   0  11  10]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.96      0.98      0.97        52\n",
      "   keepRight       0.88      0.76      0.82        50\n",
      "       yield       0.94      0.94      0.94       200\n",
      "speedLimit35       0.90      0.92      0.91       154\n",
      "speedLimit25       0.50      0.65      0.56        17\n",
      " signalAhead       0.83      0.74      0.79        47\n",
      "  pedestrian       0.96      0.99      0.97       387\n",
      "        stop       1.00      0.45      0.62        22\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       929\n",
      "   macro avg       0.87      0.80      0.82       929\n",
      "weighted avg       0.93      0.92      0.92       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9246501614639397\n",
      "********************************************************************************\n",
      "Modified Nowka:\n",
      "Found 929 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 48   2   1   0   0   0   1   0]\n",
      " [  0  48   1   0   0   0   1   0]\n",
      " [  0   2 196   2   0   0   0   0]\n",
      " [  1   0   3 147   0   0   3   0]\n",
      " [  0   0   0   0   7  10   0   0]\n",
      " [  0   0   0   1   9  37   0   0]\n",
      " [  0   1   2   2   0   1 381   0]\n",
      " [  0   0   0   1   0   0   5  16]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.98      0.92      0.95        52\n",
      "   keepRight       0.91      0.96      0.93        50\n",
      "       yield       0.97      0.98      0.97       200\n",
      "speedLimit35       0.96      0.95      0.96       154\n",
      "speedLimit25       0.44      0.41      0.42        17\n",
      " signalAhead       0.77      0.79      0.78        47\n",
      "  pedestrian       0.97      0.98      0.98       387\n",
      "        stop       1.00      0.73      0.84        22\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       929\n",
      "   macro avg       0.87      0.84      0.85       929\n",
      "weighted avg       0.95      0.95      0.95       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9472551130247578\n",
      "********************************************************************************\n",
      "Modified Nowka, no image augmentation:\n",
      "Found 929 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 52   0   0   0   0   0   0   0]\n",
      " [  0  47   1   2   0   0   0   0]\n",
      " [  0   0 199   1   0   0   0   0]\n",
      " [  0   0   0 152   0   0   2   0]\n",
      " [  0   0   0   0  14   3   0   0]\n",
      " [  0   0   0   0   0  47   0   0]\n",
      " [  0   0   3   1   0   0 383   0]\n",
      " [  0   0   0   0   0   0   2  20]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        52\n",
      "   keepRight       1.00      0.94      0.97        50\n",
      "       yield       0.98      0.99      0.99       200\n",
      "speedLimit35       0.97      0.99      0.98       154\n",
      "speedLimit25       1.00      0.82      0.90        17\n",
      " signalAhead       0.94      1.00      0.97        47\n",
      "  pedestrian       0.99      0.99      0.99       387\n",
      "        stop       1.00      0.91      0.95        22\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       929\n",
      "   macro avg       0.99      0.96      0.97       929\n",
      "weighted avg       0.98      0.98      0.98       929\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9838536060279871\n"
     ]
    }
   ],
   "source": [
    "# NOTE: models have been saved. Loading them up and making predictions.\n",
    "print('Deeper:')\n",
    "present_results(m=keras.models.load_model(deep_file),\n",
    "                image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Shallower:')\n",
    "present_results(m=keras.models.load_model(shallow_file),\n",
    "                image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Modified Nowka:')\n",
    "present_results(m=keras.models.load_model(nowka_file),\n",
    "                image_dir='validation', dg=valid_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Modified Nowka, no image augmentation:')\n",
    "present_results(m=keras.models.load_model(no_aug),\n",
    "                image_dir='validation', dg=valid_datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper:\n",
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 57   0   0   1   0   0   0   0]\n",
      " [  0  42   8   4   0   0   1   0]\n",
      " [  1   0 202   5   0   0   0   0]\n",
      " [  0   0   1 142   0   0   3   0]\n",
      " [  0   0   0   0  20   5   0   0]\n",
      " [  0   0   0   0   6  35   0   0]\n",
      " [  0   0   1   1   0   0 329   0]\n",
      " [  0   0   2   2   0   0   3  13]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.98      0.98      0.98        58\n",
      "   keepRight       1.00      0.76      0.87        55\n",
      "       yield       0.94      0.97      0.96       208\n",
      "speedLimit35       0.92      0.97      0.94       146\n",
      "speedLimit25       0.77      0.80      0.78        25\n",
      " signalAhead       0.88      0.85      0.86        41\n",
      "  pedestrian       0.98      0.99      0.99       331\n",
      "        stop       1.00      0.65      0.79        20\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       884\n",
      "   macro avg       0.93      0.87      0.90       884\n",
      "weighted avg       0.95      0.95      0.95       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9502262443438914\n",
      "********************************************************************************\n",
      "Shallower:\n",
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  39   7   8   0   0   1   0]\n",
      " [  0   1 203   3   0   0   1   0]\n",
      " [  0   1   4 134   0   0   7   0]\n",
      " [  0   0   0   0  22   3   0   0]\n",
      " [  0   0   0   0  11  27   3   0]\n",
      " [  0   0   0   0   0   0 331   0]\n",
      " [  1   0   0   1   0   0   9   9]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       0.98      1.00      0.99        58\n",
      "   keepRight       0.95      0.71      0.81        55\n",
      "       yield       0.95      0.98      0.96       208\n",
      "speedLimit35       0.92      0.92      0.92       146\n",
      "speedLimit25       0.67      0.88      0.76        25\n",
      " signalAhead       0.90      0.66      0.76        41\n",
      "  pedestrian       0.94      1.00      0.97       331\n",
      "        stop       1.00      0.45      0.62        20\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       884\n",
      "   macro avg       0.91      0.82      0.85       884\n",
      "weighted avg       0.93      0.93      0.93       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9309954751131222\n",
      "********************************************************************************\n",
      "Modified Nowka:\n",
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  49   3   2   0   0   1   0]\n",
      " [  0   3 202   3   0   0   0   0]\n",
      " [  0   1   1 138   0   0   5   1]\n",
      " [  0   0   0   0  12  13   0   0]\n",
      " [  0   0   0   1   5  35   0   0]\n",
      " [  0   0   0   1   0   0 330   0]\n",
      " [  0   0   1   1   0   0   5  13]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        58\n",
      "   keepRight       0.92      0.89      0.91        55\n",
      "       yield       0.98      0.97      0.97       208\n",
      "speedLimit35       0.95      0.95      0.95       146\n",
      "speedLimit25       0.71      0.48      0.57        25\n",
      " signalAhead       0.73      0.85      0.79        41\n",
      "  pedestrian       0.97      1.00      0.98       331\n",
      "        stop       0.93      0.65      0.76        20\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       884\n",
      "   macro avg       0.90      0.85      0.87       884\n",
      "weighted avg       0.95      0.95      0.94       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9468325791855203\n",
      "********************************************************************************\n",
      "Modified Nowka, no image augmentation:\n",
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  52   0   3   0   0   0   0]\n",
      " [  0   0 206   2   0   0   0   0]\n",
      " [  0   0   0 142   0   0   3   1]\n",
      " [  0   0   0   0  24   1   0   0]\n",
      " [  0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   0   0   0 331   0]\n",
      " [  0   0   0   0   0   0   1  19]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        58\n",
      "   keepRight       1.00      0.95      0.97        55\n",
      "       yield       1.00      0.99      1.00       208\n",
      "speedLimit35       0.97      0.97      0.97       146\n",
      "speedLimit25       1.00      0.96      0.98        25\n",
      " signalAhead       0.98      1.00      0.99        41\n",
      "  pedestrian       0.99      1.00      0.99       331\n",
      "        stop       0.95      0.95      0.95        20\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       884\n",
      "   macro avg       0.99      0.98      0.98       884\n",
      "weighted avg       0.99      0.99      0.99       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9875565610859729\n",
      "********************************************************************************\n",
      "Original Nowka:\n",
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  53   1   0   0   0   1   0]\n",
      " [  0   0 204   4   0   0   0   0]\n",
      " [  0   1   2 140   0   0   3   0]\n",
      " [  0   0   0   0  23   1   1   0]\n",
      " [  0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   2   0   0 329   0]\n",
      " [  0   0   0   1   0   0   1  18]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        58\n",
      "   keepRight       0.98      0.96      0.97        55\n",
      "       yield       0.99      0.98      0.98       208\n",
      "speedLimit35       0.95      0.96      0.96       146\n",
      "speedLimit25       1.00      0.92      0.96        25\n",
      " signalAhead       0.98      1.00      0.99        41\n",
      "  pedestrian       0.98      0.99      0.99       331\n",
      "        stop       1.00      0.90      0.95        20\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       884\n",
      "   macro avg       0.98      0.96      0.97       884\n",
      "weighted avg       0.98      0.98      0.98       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9796380090497737\n"
     ]
    }
   ],
   "source": [
    "# NOTE: models have been saved. Loading them up and making predictions.\n",
    "print('Deeper:')\n",
    "present_results(m=keras.models.load_model(deep_file),\n",
    "                image_dir='test', dg=test_datagen)\n",
    "print('*'*80)\n",
    "print('Shallower:')\n",
    "present_results(m=keras.models.load_model(shallow_file),\n",
    "                image_dir='test', dg=test_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Modified Nowka:')\n",
    "present_results(m=keras.models.load_model(nowka_file),\n",
    "                image_dir='test', dg=test_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Modified Nowka, no image augmentation:')\n",
    "present_results(m=keras.models.load_model(no_aug),\n",
    "                image_dir='test', dg=test_datagen)\n",
    "\n",
    "print('*'*80)\n",
    "print('Original Nowka:')\n",
    "present_results(m=keras.models.load_model('cnn_nowka.h5'),\n",
    "                image_dir='test', dg=test_datagen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Testing Results for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 884 images belonging to 8 classes.\n",
      "Confusion Matrix\n",
      "[[ 58   0   0   0   0   0   0   0]\n",
      " [  0  52   0   3   0   0   0   0]\n",
      " [  0   0 206   2   0   0   0   0]\n",
      " [  0   0   0 142   0   0   3   1]\n",
      " [  0   0   0   0  24   1   0   0]\n",
      " [  0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   0   0   0 331   0]\n",
      " [  0   0   0   0   0   0   1  19]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       merge       1.00      1.00      1.00        58\n",
      "   keepRight       1.00      0.95      0.97        55\n",
      "       yield       1.00      0.99      1.00       208\n",
      "speedLimit35       0.97      0.97      0.97       146\n",
      "speedLimit25       1.00      0.96      0.98        25\n",
      " signalAhead       0.98      1.00      0.99        41\n",
      "  pedestrian       0.99      1.00      0.99       331\n",
      "        stop       0.95      0.95      0.95        20\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       884\n",
      "   macro avg       0.99      0.98      0.98       884\n",
      "weighted avg       0.99      0.99      0.99       884\n",
      "\n",
      "F1 score (using average='micro')\n",
      "0.9875565610859729\n",
      "********************************************************************************\n",
      "The final model used has an identical architecture to the initial CNN\n",
      "which Professor Nowka provided. However, it was trained after fixing \n",
      "some of the training data, which was mislabeled. Also, the early_stop\n",
      "callback was used to train for longer and select the weights which \n",
      "performed best on the validation set. This model was trained strictly\n",
      "on the training set, and not on the validation or testing data.\n",
      "\n",
      "As is evident in this notebook, several different architectures and \n",
      "training techniques were attempted. As the majority of these techniques\n",
      "are self-evident by examining the notebook, I won't go into detail here.\n",
      "I will note, however, that my class weighting and image augmentation \n",
      "techniques did not work well. I believe this is indicative of the \n",
      "validation and testing sets being simply too similar to the training \n",
      "sets.\n",
      "\n",
      "It's also worth mentioning that a lot of effort went into getting this\n",
      "running on my laptop's GPU, an NVIDIA GeForce GTX 1050 Ti. I manually\n",
      "installed the NVIDIA drivers on Linux, installed some Docker utilities for\n",
      "utilizing the graphics card, and then finally ran this notebook in a \n",
      "Docker container derived from an official Tensorflow Docker image.\n"
     ]
    }
   ],
   "source": [
    "present_results(m=keras.models.load_model(no_aug), image_dir='test', dg=test_datagen)\n",
    "print('*'*80)\n",
    "print('The final model used has an identical architecture to the initial CNN')\n",
    "print('which Professor Nowka provided. However, it was trained after fixing ')\n",
    "print('some of the training data, which was mislabeled. Also, the early_stop')\n",
    "print('callback was used to train for longer and select the weights which ')\n",
    "print('performed best on the validation set. This model was trained strictly')\n",
    "print('on the training set, and not on the validation or testing data.')\n",
    "print('')\n",
    "print('As is evident in this notebook, several different architectures and ')\n",
    "print('training techniques were attempted. As the majority of these techniques')\n",
    "print('are self-evident by examining the notebook, I won\\'t go into detail here.')\n",
    "print('I will note, however, that my class weighting and image augmentation ')\n",
    "print('techniques did not work well. I believe this is indicative of the ')\n",
    "print('validation and testing sets being simply too similar to the training ')\n",
    "print('sets.')\n",
    "print('')\n",
    "print('It\\'s also worth mentioning that a lot of effort went into getting this')\n",
    "print('running on my laptop\\'s GPU, an NVIDIA GeForce GTX 1050 Ti. I manually')\n",
    "print('installed the NVIDIA drivers on Linux, installed some Docker utilities for')\n",
    "print('utilizing the graphics card, and then finally ran this notebook in a ')\n",
    "print('Docker container derived from an official Tensorflow Docker image.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
