{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECEN 489 Python Exercise 3\n",
    "## Brandon Thayer, Spring 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Avoid truncation when using 'describe'\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set figure size for plotting.\n",
    "plt.rcParams['figure.figsize'] = [20, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Helper function for training and predicting.\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def train_predict(model, x_train, y_train, x_test, y_test, c_str):\n",
    "    \"\"\"Helper for performing training and predictions.\n",
    "\n",
    "    :param model: Initialized classifier. E.g. GaussianNb()\n",
    "    :param x_train: Training data.\n",
    "    :param y_train: Training labels.\n",
    "    :param x_test: Testing data.\n",
    "    :param y_test: Testing labels.\n",
    "    :param c_str: String for classifier. E.g. \"Naive Bayes\"\n",
    "    :returns: confusion matrix and F1 score.\n",
    "    \"\"\"\n",
    "    # Train the model.\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict.\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Generate confusion matrix.\n",
    "    cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Compute F1 score.\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Display.\n",
    "    print('*' * 80)\n",
    "    print(c_str)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print('F1 score for testing data: {:.4f}'.format(f1))\n",
    "    print('Training accuracy: {:.4f}'.format(model.score(x_train, y_train)))\n",
    "    print('Testing accuracy: {:.4f}'.format(model.score(x_test, y_test)))\n",
    "    print('*' * 80)\n",
    "\n",
    "    return cm, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Load and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26947 entries, 0 to 26946\n",
      "Data columns (total 27 columns):\n",
      "Feature00    23759 non-null float64\n",
      "Feature01    24288 non-null float64\n",
      "Feature02    6395 non-null float64\n",
      "Feature03    6565 non-null float64\n",
      "Feature04    26947 non-null object\n",
      "Feature05    3115 non-null float64\n",
      "Feature06    3201 non-null float64\n",
      "Feature07    3197 non-null float64\n",
      "Feature08    3201 non-null float64\n",
      "Feature09    3362 non-null float64\n",
      "Feature10    3362 non-null float64\n",
      "Feature11    3362 non-null float64\n",
      "Feature12    3195 non-null float64\n",
      "Feature13    3197 non-null float64\n",
      "Feature14    3340 non-null float64\n",
      "Feature15    3202 non-null float64\n",
      "Feature16    3202 non-null float64\n",
      "Feature17    3202 non-null float64\n",
      "Feature18    3362 non-null float64\n",
      "Feature19    3362 non-null float64\n",
      "Feature20    3362 non-null float64\n",
      "Feature21    3195 non-null float64\n",
      "Feature22    3202 non-null float64\n",
      "Feature23    8 non-null float64\n",
      "Feature24    3202 non-null float64\n",
      "Feature25    3202 non-null float64\n",
      "Label        26947 non-null int64\n",
      "dtypes: float64(25), int64(1), object(1)\n",
      "memory usage: 5.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature00</th>\n",
       "      <th>Feature01</th>\n",
       "      <th>Feature02</th>\n",
       "      <th>Feature03</th>\n",
       "      <th>Feature05</th>\n",
       "      <th>Feature06</th>\n",
       "      <th>Feature07</th>\n",
       "      <th>Feature08</th>\n",
       "      <th>Feature09</th>\n",
       "      <th>Feature10</th>\n",
       "      <th>Feature11</th>\n",
       "      <th>Feature12</th>\n",
       "      <th>Feature13</th>\n",
       "      <th>Feature14</th>\n",
       "      <th>Feature15</th>\n",
       "      <th>Feature16</th>\n",
       "      <th>Feature17</th>\n",
       "      <th>Feature18</th>\n",
       "      <th>Feature19</th>\n",
       "      <th>Feature20</th>\n",
       "      <th>Feature21</th>\n",
       "      <th>Feature22</th>\n",
       "      <th>Feature23</th>\n",
       "      <th>Feature24</th>\n",
       "      <th>Feature25</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23759.000000</td>\n",
       "      <td>2.428800e+04</td>\n",
       "      <td>6395.000000</td>\n",
       "      <td>6565.000000</td>\n",
       "      <td>3115.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3.197000e+03</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3197.000000</td>\n",
       "      <td>3340.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3.202000e+03</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>26947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.764512</td>\n",
       "      <td>3.625664e+05</td>\n",
       "      <td>62.151352</td>\n",
       "      <td>9.116638</td>\n",
       "      <td>8.550284</td>\n",
       "      <td>105.908008</td>\n",
       "      <td>1.887889e+05</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>670.588797</td>\n",
       "      <td>2191.221674</td>\n",
       "      <td>1190.382120</td>\n",
       "      <td>39.753154</td>\n",
       "      <td>3.025138</td>\n",
       "      <td>0.119516</td>\n",
       "      <td>177.615993</td>\n",
       "      <td>8.093276e+04</td>\n",
       "      <td>0.325374</td>\n",
       "      <td>303.498199</td>\n",
       "      <td>1038.740703</td>\n",
       "      <td>569.128142</td>\n",
       "      <td>30.561336</td>\n",
       "      <td>3.231875</td>\n",
       "      <td>0.225650</td>\n",
       "      <td>95.265756</td>\n",
       "      <td>45.823957</td>\n",
       "      <td>0.256912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.641441</td>\n",
       "      <td>5.650321e+07</td>\n",
       "      <td>29.252099</td>\n",
       "      <td>0.521882</td>\n",
       "      <td>0.807067</td>\n",
       "      <td>135.535239</td>\n",
       "      <td>1.067450e+07</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>61.265761</td>\n",
       "      <td>25988.384476</td>\n",
       "      <td>78.180725</td>\n",
       "      <td>2.372065</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>27.270181</td>\n",
       "      <td>4.579664e+06</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>65.533392</td>\n",
       "      <td>20340.841285</td>\n",
       "      <td>80.184771</td>\n",
       "      <td>2.594434</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.233881</td>\n",
       "      <td>19.246906</td>\n",
       "      <td>5.560062</td>\n",
       "      <td>0.436938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.054714</td>\n",
       "      <td>5.774274e+00</td>\n",
       "      <td>24.440578</td>\n",
       "      <td>7.642299</td>\n",
       "      <td>5.572053</td>\n",
       "      <td>26.645001</td>\n",
       "      <td>7.808669e-02</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>30.602777</td>\n",
       "      <td>58.722225</td>\n",
       "      <td>48.024998</td>\n",
       "      <td>26.568422</td>\n",
       "      <td>2.786466</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>55.760000</td>\n",
       "      <td>5.311932e-02</td>\n",
       "      <td>0.209769</td>\n",
       "      <td>4.551250</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>7.652778</td>\n",
       "      <td>10.361442</td>\n",
       "      <td>3.058215</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>60.245002</td>\n",
       "      <td>0.806450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.085374</td>\n",
       "      <td>8.175224e+00</td>\n",
       "      <td>41.069887</td>\n",
       "      <td>8.737444</td>\n",
       "      <td>8.004584</td>\n",
       "      <td>51.185001</td>\n",
       "      <td>1.784541e-01</td>\n",
       "      <td>0.291138</td>\n",
       "      <td>631.111145</td>\n",
       "      <td>368.847214</td>\n",
       "      <td>1141.388794</td>\n",
       "      <td>38.286371</td>\n",
       "      <td>3.000036</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>165.200007</td>\n",
       "      <td>2.521315e-01</td>\n",
       "      <td>0.322766</td>\n",
       "      <td>276.645828</td>\n",
       "      <td>53.525000</td>\n",
       "      <td>531.201401</td>\n",
       "      <td>28.983149</td>\n",
       "      <td>3.209533</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>87.326252</td>\n",
       "      <td>42.486250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.514414</td>\n",
       "      <td>8.544922e+00</td>\n",
       "      <td>53.998638</td>\n",
       "      <td>9.072266</td>\n",
       "      <td>8.538931</td>\n",
       "      <td>73.570001</td>\n",
       "      <td>1.851615e-01</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>665.131958</td>\n",
       "      <td>645.000000</td>\n",
       "      <td>1186.111084</td>\n",
       "      <td>39.862242</td>\n",
       "      <td>3.021357</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>175.800000</td>\n",
       "      <td>2.568918e-01</td>\n",
       "      <td>0.325444</td>\n",
       "      <td>304.333344</td>\n",
       "      <td>111.694443</td>\n",
       "      <td>570.555542</td>\n",
       "      <td>30.607371</td>\n",
       "      <td>3.226834</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>94.039999</td>\n",
       "      <td>45.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.199785</td>\n",
       "      <td>8.931361e+00</td>\n",
       "      <td>74.954861</td>\n",
       "      <td>9.455915</td>\n",
       "      <td>9.096312</td>\n",
       "      <td>117.700001</td>\n",
       "      <td>1.908521e-01</td>\n",
       "      <td>0.296182</td>\n",
       "      <td>702.482620</td>\n",
       "      <td>1098.472260</td>\n",
       "      <td>1233.541626</td>\n",
       "      <td>41.247956</td>\n",
       "      <td>3.050236</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>187.450001</td>\n",
       "      <td>2.615333e-01</td>\n",
       "      <td>0.328006</td>\n",
       "      <td>329.013886</td>\n",
       "      <td>231.819439</td>\n",
       "      <td>607.499939</td>\n",
       "      <td>32.102097</td>\n",
       "      <td>3.252851</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>101.250000</td>\n",
       "      <td>49.188749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.268906</td>\n",
       "      <td>8.805803e+09</td>\n",
       "      <td>205.078095</td>\n",
       "      <td>10.906808</td>\n",
       "      <td>11.476805</td>\n",
       "      <td>5482.499953</td>\n",
       "      <td>6.035576e+08</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>1115.416626</td>\n",
       "      <td>567472.250000</td>\n",
       "      <td>1680.277832</td>\n",
       "      <td>62.972978</td>\n",
       "      <td>3.169298</td>\n",
       "      <td>1.499800</td>\n",
       "      <td>740.149990</td>\n",
       "      <td>2.591459e+08</td>\n",
       "      <td>0.341348</td>\n",
       "      <td>2817.500000</td>\n",
       "      <td>555666.687500</td>\n",
       "      <td>3332.500000</td>\n",
       "      <td>48.439015</td>\n",
       "      <td>4.006091</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>566.000002</td>\n",
       "      <td>123.299993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Feature00     Feature01    Feature02    Feature03    Feature05  \\\n",
       "count  23759.000000  2.428800e+04  6395.000000  6565.000000  3115.000000   \n",
       "mean      34.764512  3.625664e+05    62.151352     9.116638     8.550284   \n",
       "std       18.641441  5.650321e+07    29.252099     0.521882     0.807067   \n",
       "min       14.054714  5.774274e+00    24.440578     7.642299     5.572053   \n",
       "25%       22.085374  8.175224e+00    41.069887     8.737444     8.004584   \n",
       "50%       28.514414  8.544922e+00    53.998638     9.072266     8.538931   \n",
       "75%       41.199785  8.931361e+00    74.954861     9.455915     9.096312   \n",
       "max      142.268906  8.805803e+09   205.078095    10.906808    11.476805   \n",
       "\n",
       "         Feature06     Feature07    Feature08    Feature09      Feature10  \\\n",
       "count  3201.000000  3.197000e+03  3201.000000  3362.000000    3362.000000   \n",
       "mean    105.908008  1.887889e+05     0.293488   670.588797    2191.221674   \n",
       "std     135.535239  1.067450e+07     0.009112    61.265761   25988.384476   \n",
       "min      26.645001  7.808669e-02     0.051327    30.602777      58.722225   \n",
       "25%      51.185001  1.784541e-01     0.291138   631.111145     368.847214   \n",
       "50%      73.570001  1.851615e-01     0.294200   665.131958     645.000000   \n",
       "75%     117.700001  1.908521e-01     0.296182   702.482620    1098.472260   \n",
       "max    5482.499953  6.035576e+08     0.309033  1115.416626  567472.250000   \n",
       "\n",
       "         Feature11    Feature12    Feature13    Feature14    Feature15  \\\n",
       "count  3362.000000  3195.000000  3197.000000  3340.000000  3202.000000   \n",
       "mean   1190.382120    39.753154     3.025138     0.119516   177.615993   \n",
       "std      78.180725     2.372065     0.036748     0.059998    27.270181   \n",
       "min      48.024998    26.568422     2.786466     0.000700    55.760000   \n",
       "25%    1141.388794    38.286371     3.000036     0.096900   165.200007   \n",
       "50%    1186.111084    39.862242     3.021357     0.122100   175.800000   \n",
       "75%    1233.541626    41.247956     3.050236     0.143100   187.450001   \n",
       "max    1680.277832    62.972978     3.169298     1.499800   740.149990   \n",
       "\n",
       "          Feature16    Feature17    Feature18      Feature19    Feature20  \\\n",
       "count  3.202000e+03  3202.000000  3362.000000    3362.000000  3362.000000   \n",
       "mean   8.093276e+04     0.325374   303.498199    1038.740703   569.128142   \n",
       "std    4.579664e+06     0.005560    65.533392   20340.841285    80.184771   \n",
       "min    5.311932e-02     0.209769     4.551250       0.458333     7.652778   \n",
       "25%    2.521315e-01     0.322766   276.645828      53.525000   531.201401   \n",
       "50%    2.568918e-01     0.325444   304.333344     111.694443   570.555542   \n",
       "75%    2.615333e-01     0.328006   329.013886     231.819439   607.499939   \n",
       "max    2.591459e+08     0.341348  2817.500000  555666.687500  3332.500000   \n",
       "\n",
       "         Feature21    Feature22  Feature23    Feature24    Feature25  \\\n",
       "count  3195.000000  3202.000000   8.000000  3202.000000  3202.000000   \n",
       "mean     30.561336     3.231875   0.225650    95.265756    45.823957   \n",
       "std       2.594434     0.034437   0.233881    19.246906     5.560062   \n",
       "min      10.361442     3.058215   0.002000    60.245002     0.806450   \n",
       "25%      28.983149     3.209533   0.012875    87.326252    42.486250   \n",
       "50%      30.607371     3.226834   0.141350    94.039999    45.735000   \n",
       "75%      32.102097     3.252851   0.499800   101.250000    49.188749   \n",
       "max      48.439015     4.006091   0.499800   566.000002   123.299993   \n",
       "\n",
       "              Label  \n",
       "count  26947.000000  \n",
       "mean       0.256912  \n",
       "std        0.436938  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ecen489py3data.csv')\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Text attributes to numeric attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26947 entries, 0 to 26946\n",
      "Data columns (total 27 columns):\n",
      "Feature00    23759 non-null float64\n",
      "Feature01    24288 non-null float64\n",
      "Feature02    6395 non-null float64\n",
      "Feature03    6565 non-null float64\n",
      "Feature04    26947 non-null int8\n",
      "Feature05    3115 non-null float64\n",
      "Feature06    3201 non-null float64\n",
      "Feature07    3197 non-null float64\n",
      "Feature08    3201 non-null float64\n",
      "Feature09    3362 non-null float64\n",
      "Feature10    3362 non-null float64\n",
      "Feature11    3362 non-null float64\n",
      "Feature12    3195 non-null float64\n",
      "Feature13    3197 non-null float64\n",
      "Feature14    3340 non-null float64\n",
      "Feature15    3202 non-null float64\n",
      "Feature16    3202 non-null float64\n",
      "Feature17    3202 non-null float64\n",
      "Feature18    3362 non-null float64\n",
      "Feature19    3362 non-null float64\n",
      "Feature20    3362 non-null float64\n",
      "Feature21    3195 non-null float64\n",
      "Feature22    3202 non-null float64\n",
      "Feature23    8 non-null float64\n",
      "Feature24    3202 non-null float64\n",
      "Feature25    3202 non-null float64\n",
      "Label        26947 non-null int64\n",
      "dtypes: float64(25), int64(1), int8(1)\n",
      "memory usage: 5.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature00</th>\n",
       "      <th>Feature01</th>\n",
       "      <th>Feature02</th>\n",
       "      <th>Feature03</th>\n",
       "      <th>Feature04</th>\n",
       "      <th>Feature05</th>\n",
       "      <th>Feature06</th>\n",
       "      <th>Feature07</th>\n",
       "      <th>Feature08</th>\n",
       "      <th>Feature09</th>\n",
       "      <th>Feature10</th>\n",
       "      <th>Feature11</th>\n",
       "      <th>Feature12</th>\n",
       "      <th>Feature13</th>\n",
       "      <th>Feature14</th>\n",
       "      <th>Feature15</th>\n",
       "      <th>Feature16</th>\n",
       "      <th>Feature17</th>\n",
       "      <th>Feature18</th>\n",
       "      <th>Feature19</th>\n",
       "      <th>Feature20</th>\n",
       "      <th>Feature21</th>\n",
       "      <th>Feature22</th>\n",
       "      <th>Feature23</th>\n",
       "      <th>Feature24</th>\n",
       "      <th>Feature25</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23759.000000</td>\n",
       "      <td>2.428800e+04</td>\n",
       "      <td>6395.000000</td>\n",
       "      <td>6565.000000</td>\n",
       "      <td>26947.000000</td>\n",
       "      <td>3115.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3.197000e+03</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3197.000000</td>\n",
       "      <td>3340.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3.202000e+03</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>26947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.764512</td>\n",
       "      <td>3.625664e+05</td>\n",
       "      <td>62.151352</td>\n",
       "      <td>9.116638</td>\n",
       "      <td>17.630979</td>\n",
       "      <td>8.550284</td>\n",
       "      <td>105.908008</td>\n",
       "      <td>1.887889e+05</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>670.588797</td>\n",
       "      <td>2191.221674</td>\n",
       "      <td>1190.382120</td>\n",
       "      <td>39.753154</td>\n",
       "      <td>3.025138</td>\n",
       "      <td>0.119516</td>\n",
       "      <td>177.615993</td>\n",
       "      <td>8.093276e+04</td>\n",
       "      <td>0.325374</td>\n",
       "      <td>303.498199</td>\n",
       "      <td>1038.740703</td>\n",
       "      <td>569.128142</td>\n",
       "      <td>30.561336</td>\n",
       "      <td>3.231875</td>\n",
       "      <td>0.225650</td>\n",
       "      <td>95.265756</td>\n",
       "      <td>45.823957</td>\n",
       "      <td>0.256912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.641441</td>\n",
       "      <td>5.650321e+07</td>\n",
       "      <td>29.252099</td>\n",
       "      <td>0.521882</td>\n",
       "      <td>8.933940</td>\n",
       "      <td>0.807067</td>\n",
       "      <td>135.535239</td>\n",
       "      <td>1.067450e+07</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>61.265761</td>\n",
       "      <td>25988.384476</td>\n",
       "      <td>78.180725</td>\n",
       "      <td>2.372065</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>27.270181</td>\n",
       "      <td>4.579664e+06</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>65.533392</td>\n",
       "      <td>20340.841285</td>\n",
       "      <td>80.184771</td>\n",
       "      <td>2.594434</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.233881</td>\n",
       "      <td>19.246906</td>\n",
       "      <td>5.560062</td>\n",
       "      <td>0.436938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.054714</td>\n",
       "      <td>5.774274e+00</td>\n",
       "      <td>24.440578</td>\n",
       "      <td>7.642299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.572053</td>\n",
       "      <td>26.645001</td>\n",
       "      <td>7.808669e-02</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>30.602777</td>\n",
       "      <td>58.722225</td>\n",
       "      <td>48.024998</td>\n",
       "      <td>26.568422</td>\n",
       "      <td>2.786466</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>55.760000</td>\n",
       "      <td>5.311932e-02</td>\n",
       "      <td>0.209769</td>\n",
       "      <td>4.551250</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>7.652778</td>\n",
       "      <td>10.361442</td>\n",
       "      <td>3.058215</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>60.245002</td>\n",
       "      <td>0.806450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.085374</td>\n",
       "      <td>8.175224e+00</td>\n",
       "      <td>41.069887</td>\n",
       "      <td>8.737444</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.004584</td>\n",
       "      <td>51.185001</td>\n",
       "      <td>1.784541e-01</td>\n",
       "      <td>0.291138</td>\n",
       "      <td>631.111145</td>\n",
       "      <td>368.847214</td>\n",
       "      <td>1141.388794</td>\n",
       "      <td>38.286371</td>\n",
       "      <td>3.000036</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>165.200007</td>\n",
       "      <td>2.521315e-01</td>\n",
       "      <td>0.322766</td>\n",
       "      <td>276.645828</td>\n",
       "      <td>53.525000</td>\n",
       "      <td>531.201401</td>\n",
       "      <td>28.983149</td>\n",
       "      <td>3.209533</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>87.326252</td>\n",
       "      <td>42.486250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.514414</td>\n",
       "      <td>8.544922e+00</td>\n",
       "      <td>53.998638</td>\n",
       "      <td>9.072266</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>8.538931</td>\n",
       "      <td>73.570001</td>\n",
       "      <td>1.851615e-01</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>665.131958</td>\n",
       "      <td>645.000000</td>\n",
       "      <td>1186.111084</td>\n",
       "      <td>39.862242</td>\n",
       "      <td>3.021357</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>175.800000</td>\n",
       "      <td>2.568918e-01</td>\n",
       "      <td>0.325444</td>\n",
       "      <td>304.333344</td>\n",
       "      <td>111.694443</td>\n",
       "      <td>570.555542</td>\n",
       "      <td>30.607371</td>\n",
       "      <td>3.226834</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>94.039999</td>\n",
       "      <td>45.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.199785</td>\n",
       "      <td>8.931361e+00</td>\n",
       "      <td>74.954861</td>\n",
       "      <td>9.455915</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>9.096312</td>\n",
       "      <td>117.700001</td>\n",
       "      <td>1.908521e-01</td>\n",
       "      <td>0.296182</td>\n",
       "      <td>702.482620</td>\n",
       "      <td>1098.472260</td>\n",
       "      <td>1233.541626</td>\n",
       "      <td>41.247956</td>\n",
       "      <td>3.050236</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>187.450001</td>\n",
       "      <td>2.615333e-01</td>\n",
       "      <td>0.328006</td>\n",
       "      <td>329.013886</td>\n",
       "      <td>231.819439</td>\n",
       "      <td>607.499939</td>\n",
       "      <td>32.102097</td>\n",
       "      <td>3.252851</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>101.250000</td>\n",
       "      <td>49.188749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.268906</td>\n",
       "      <td>8.805803e+09</td>\n",
       "      <td>205.078095</td>\n",
       "      <td>10.906808</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>11.476805</td>\n",
       "      <td>5482.499953</td>\n",
       "      <td>6.035576e+08</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>1115.416626</td>\n",
       "      <td>567472.250000</td>\n",
       "      <td>1680.277832</td>\n",
       "      <td>62.972978</td>\n",
       "      <td>3.169298</td>\n",
       "      <td>1.499800</td>\n",
       "      <td>740.149990</td>\n",
       "      <td>2.591459e+08</td>\n",
       "      <td>0.341348</td>\n",
       "      <td>2817.500000</td>\n",
       "      <td>555666.687500</td>\n",
       "      <td>3332.500000</td>\n",
       "      <td>48.439015</td>\n",
       "      <td>4.006091</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>566.000002</td>\n",
       "      <td>123.299993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Feature00     Feature01    Feature02    Feature03     Feature04  \\\n",
       "count  23759.000000  2.428800e+04  6395.000000  6565.000000  26947.000000   \n",
       "mean      34.764512  3.625664e+05    62.151352     9.116638     17.630979   \n",
       "std       18.641441  5.650321e+07    29.252099     0.521882      8.933940   \n",
       "min       14.054714  5.774274e+00    24.440578     7.642299      0.000000   \n",
       "25%       22.085374  8.175224e+00    41.069887     8.737444     10.000000   \n",
       "50%       28.514414  8.544922e+00    53.998638     9.072266     22.000000   \n",
       "75%       41.199785  8.931361e+00    74.954861     9.455915     27.000000   \n",
       "max      142.268906  8.805803e+09   205.078095    10.906808     27.000000   \n",
       "\n",
       "         Feature05    Feature06     Feature07    Feature08    Feature09  \\\n",
       "count  3115.000000  3201.000000  3.197000e+03  3201.000000  3362.000000   \n",
       "mean      8.550284   105.908008  1.887889e+05     0.293488   670.588797   \n",
       "std       0.807067   135.535239  1.067450e+07     0.009112    61.265761   \n",
       "min       5.572053    26.645001  7.808669e-02     0.051327    30.602777   \n",
       "25%       8.004584    51.185001  1.784541e-01     0.291138   631.111145   \n",
       "50%       8.538931    73.570001  1.851615e-01     0.294200   665.131958   \n",
       "75%       9.096312   117.700001  1.908521e-01     0.296182   702.482620   \n",
       "max      11.476805  5482.499953  6.035576e+08     0.309033  1115.416626   \n",
       "\n",
       "           Feature10    Feature11    Feature12    Feature13    Feature14  \\\n",
       "count    3362.000000  3362.000000  3195.000000  3197.000000  3340.000000   \n",
       "mean     2191.221674  1190.382120    39.753154     3.025138     0.119516   \n",
       "std     25988.384476    78.180725     2.372065     0.036748     0.059998   \n",
       "min        58.722225    48.024998    26.568422     2.786466     0.000700   \n",
       "25%       368.847214  1141.388794    38.286371     3.000036     0.096900   \n",
       "50%       645.000000  1186.111084    39.862242     3.021357     0.122100   \n",
       "75%      1098.472260  1233.541626    41.247956     3.050236     0.143100   \n",
       "max    567472.250000  1680.277832    62.972978     3.169298     1.499800   \n",
       "\n",
       "         Feature15     Feature16    Feature17    Feature18      Feature19  \\\n",
       "count  3202.000000  3.202000e+03  3202.000000  3362.000000    3362.000000   \n",
       "mean    177.615993  8.093276e+04     0.325374   303.498199    1038.740703   \n",
       "std      27.270181  4.579664e+06     0.005560    65.533392   20340.841285   \n",
       "min      55.760000  5.311932e-02     0.209769     4.551250       0.458333   \n",
       "25%     165.200007  2.521315e-01     0.322766   276.645828      53.525000   \n",
       "50%     175.800000  2.568918e-01     0.325444   304.333344     111.694443   \n",
       "75%     187.450001  2.615333e-01     0.328006   329.013886     231.819439   \n",
       "max     740.149990  2.591459e+08     0.341348  2817.500000  555666.687500   \n",
       "\n",
       "         Feature20    Feature21    Feature22  Feature23    Feature24  \\\n",
       "count  3362.000000  3195.000000  3202.000000   8.000000  3202.000000   \n",
       "mean    569.128142    30.561336     3.231875   0.225650    95.265756   \n",
       "std      80.184771     2.594434     0.034437   0.233881    19.246906   \n",
       "min       7.652778    10.361442     3.058215   0.002000    60.245002   \n",
       "25%     531.201401    28.983149     3.209533   0.012875    87.326252   \n",
       "50%     570.555542    30.607371     3.226834   0.141350    94.039999   \n",
       "75%     607.499939    32.102097     3.252851   0.499800   101.250000   \n",
       "max    3332.500000    48.439015     4.006091   0.499800   566.000002   \n",
       "\n",
       "         Feature25         Label  \n",
       "count  3202.000000  26947.000000  \n",
       "mean     45.823957      0.256912  \n",
       "std       5.560062      0.436938  \n",
       "min       0.806450      0.000000  \n",
       "25%      42.486250      0.000000  \n",
       "50%      45.735000      0.000000  \n",
       "75%      49.188749      1.000000  \n",
       "max     123.299993      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from Prof. Nowka to cast 'object' data types to categorical.\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtype == 'object':\n",
    "        df[col_name] = df[col_name].astype('category')\n",
    "        df[col_name] = df[col_name].cat.codes\n",
    "        \n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Fix Bad Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature00</th>\n",
       "      <th>Feature01</th>\n",
       "      <th>Feature02</th>\n",
       "      <th>Feature03</th>\n",
       "      <th>Feature04</th>\n",
       "      <th>Feature05</th>\n",
       "      <th>Feature06</th>\n",
       "      <th>Feature07</th>\n",
       "      <th>Feature08</th>\n",
       "      <th>Feature09</th>\n",
       "      <th>Feature10</th>\n",
       "      <th>Feature11</th>\n",
       "      <th>Feature12</th>\n",
       "      <th>Feature13</th>\n",
       "      <th>Feature14</th>\n",
       "      <th>Feature15</th>\n",
       "      <th>Feature16</th>\n",
       "      <th>Feature17</th>\n",
       "      <th>Feature18</th>\n",
       "      <th>Feature19</th>\n",
       "      <th>Feature20</th>\n",
       "      <th>Feature21</th>\n",
       "      <th>Feature22</th>\n",
       "      <th>Feature23</th>\n",
       "      <th>Feature24</th>\n",
       "      <th>Feature25</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23759.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>6395.000000</td>\n",
       "      <td>6565.000000</td>\n",
       "      <td>26946.000000</td>\n",
       "      <td>3115.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3196.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3197.000000</td>\n",
       "      <td>3340.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3201.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3362.000000</td>\n",
       "      <td>3195.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>3202.000000</td>\n",
       "      <td>26946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>34.764512</td>\n",
       "      <td>8.576644</td>\n",
       "      <td>62.151352</td>\n",
       "      <td>9.116638</td>\n",
       "      <td>17.630632</td>\n",
       "      <td>8.550284</td>\n",
       "      <td>105.908008</td>\n",
       "      <td>0.184914</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>670.588797</td>\n",
       "      <td>2191.221674</td>\n",
       "      <td>1190.382120</td>\n",
       "      <td>39.753154</td>\n",
       "      <td>3.025138</td>\n",
       "      <td>0.119516</td>\n",
       "      <td>177.615993</td>\n",
       "      <td>0.256455</td>\n",
       "      <td>0.325374</td>\n",
       "      <td>303.498199</td>\n",
       "      <td>1038.740703</td>\n",
       "      <td>569.128142</td>\n",
       "      <td>30.561336</td>\n",
       "      <td>3.231875</td>\n",
       "      <td>0.225650</td>\n",
       "      <td>95.265756</td>\n",
       "      <td>45.823957</td>\n",
       "      <td>0.256921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.641441</td>\n",
       "      <td>0.551513</td>\n",
       "      <td>29.252099</td>\n",
       "      <td>0.521882</td>\n",
       "      <td>8.933923</td>\n",
       "      <td>0.807067</td>\n",
       "      <td>135.535239</td>\n",
       "      <td>0.019106</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>61.265761</td>\n",
       "      <td>25988.384476</td>\n",
       "      <td>78.180725</td>\n",
       "      <td>2.372065</td>\n",
       "      <td>0.036748</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>27.270181</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>65.533392</td>\n",
       "      <td>20340.841285</td>\n",
       "      <td>80.184771</td>\n",
       "      <td>2.594434</td>\n",
       "      <td>0.034437</td>\n",
       "      <td>0.233881</td>\n",
       "      <td>19.246906</td>\n",
       "      <td>5.560062</td>\n",
       "      <td>0.436944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.054714</td>\n",
       "      <td>5.774274</td>\n",
       "      <td>24.440578</td>\n",
       "      <td>7.642299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.572053</td>\n",
       "      <td>26.645001</td>\n",
       "      <td>0.078087</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>30.602777</td>\n",
       "      <td>58.722225</td>\n",
       "      <td>48.024998</td>\n",
       "      <td>26.568422</td>\n",
       "      <td>2.786466</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>55.760000</td>\n",
       "      <td>0.053119</td>\n",
       "      <td>0.209769</td>\n",
       "      <td>4.551250</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>7.652778</td>\n",
       "      <td>10.361442</td>\n",
       "      <td>3.058215</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>60.245002</td>\n",
       "      <td>0.806450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.085374</td>\n",
       "      <td>8.175224</td>\n",
       "      <td>41.069887</td>\n",
       "      <td>8.737444</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.004584</td>\n",
       "      <td>51.185001</td>\n",
       "      <td>0.178450</td>\n",
       "      <td>0.291138</td>\n",
       "      <td>631.111145</td>\n",
       "      <td>368.847214</td>\n",
       "      <td>1141.388794</td>\n",
       "      <td>38.286371</td>\n",
       "      <td>3.000036</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>165.200007</td>\n",
       "      <td>0.252126</td>\n",
       "      <td>0.322766</td>\n",
       "      <td>276.645828</td>\n",
       "      <td>53.525000</td>\n",
       "      <td>531.201401</td>\n",
       "      <td>28.983149</td>\n",
       "      <td>3.209533</td>\n",
       "      <td>0.012875</td>\n",
       "      <td>87.326252</td>\n",
       "      <td>42.486250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.514414</td>\n",
       "      <td>8.544922</td>\n",
       "      <td>53.998638</td>\n",
       "      <td>9.072266</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>8.538931</td>\n",
       "      <td>73.570001</td>\n",
       "      <td>0.185146</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>665.131958</td>\n",
       "      <td>645.000000</td>\n",
       "      <td>1186.111084</td>\n",
       "      <td>39.862242</td>\n",
       "      <td>3.021357</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>175.800000</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.325444</td>\n",
       "      <td>304.333344</td>\n",
       "      <td>111.694443</td>\n",
       "      <td>570.555542</td>\n",
       "      <td>30.607371</td>\n",
       "      <td>3.226834</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>94.039999</td>\n",
       "      <td>45.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.199785</td>\n",
       "      <td>8.931361</td>\n",
       "      <td>74.954861</td>\n",
       "      <td>9.455915</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>9.096312</td>\n",
       "      <td>117.700001</td>\n",
       "      <td>0.190851</td>\n",
       "      <td>0.296182</td>\n",
       "      <td>702.482620</td>\n",
       "      <td>1098.472260</td>\n",
       "      <td>1233.541626</td>\n",
       "      <td>41.247956</td>\n",
       "      <td>3.050236</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>187.450001</td>\n",
       "      <td>0.261532</td>\n",
       "      <td>0.328006</td>\n",
       "      <td>329.013886</td>\n",
       "      <td>231.819439</td>\n",
       "      <td>607.499939</td>\n",
       "      <td>32.102097</td>\n",
       "      <td>3.252851</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>101.250000</td>\n",
       "      <td>49.188749</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.268906</td>\n",
       "      <td>11.309989</td>\n",
       "      <td>205.078095</td>\n",
       "      <td>10.906808</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>11.476805</td>\n",
       "      <td>5482.499953</td>\n",
       "      <td>0.603558</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>1115.416626</td>\n",
       "      <td>567472.250000</td>\n",
       "      <td>1680.277832</td>\n",
       "      <td>62.972978</td>\n",
       "      <td>3.169298</td>\n",
       "      <td>1.499800</td>\n",
       "      <td>740.149990</td>\n",
       "      <td>0.968355</td>\n",
       "      <td>0.341348</td>\n",
       "      <td>2817.500000</td>\n",
       "      <td>555666.687500</td>\n",
       "      <td>3332.500000</td>\n",
       "      <td>48.439015</td>\n",
       "      <td>4.006091</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>566.000002</td>\n",
       "      <td>123.299993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Feature00     Feature01    Feature02    Feature03     Feature04  \\\n",
       "count  23759.000000  24287.000000  6395.000000  6565.000000  26946.000000   \n",
       "mean      34.764512      8.576644    62.151352     9.116638     17.630632   \n",
       "std       18.641441      0.551513    29.252099     0.521882      8.933923   \n",
       "min       14.054714      5.774274    24.440578     7.642299      0.000000   \n",
       "25%       22.085374      8.175224    41.069887     8.737444     10.000000   \n",
       "50%       28.514414      8.544922    53.998638     9.072266     22.000000   \n",
       "75%       41.199785      8.931361    74.954861     9.455915     27.000000   \n",
       "max      142.268906     11.309989   205.078095    10.906808     27.000000   \n",
       "\n",
       "         Feature05    Feature06    Feature07    Feature08    Feature09  \\\n",
       "count  3115.000000  3201.000000  3196.000000  3201.000000  3362.000000   \n",
       "mean      8.550284   105.908008     0.184914     0.293488   670.588797   \n",
       "std       0.807067   135.535239     0.019106     0.009112    61.265761   \n",
       "min       5.572053    26.645001     0.078087     0.051327    30.602777   \n",
       "25%       8.004584    51.185001     0.178450     0.291138   631.111145   \n",
       "50%       8.538931    73.570001     0.185146     0.294200   665.131958   \n",
       "75%       9.096312   117.700001     0.190851     0.296182   702.482620   \n",
       "max      11.476805  5482.499953     0.603558     0.309033  1115.416626   \n",
       "\n",
       "           Feature10    Feature11    Feature12    Feature13    Feature14  \\\n",
       "count    3362.000000  3362.000000  3195.000000  3197.000000  3340.000000   \n",
       "mean     2191.221674  1190.382120    39.753154     3.025138     0.119516   \n",
       "std     25988.384476    78.180725     2.372065     0.036748     0.059998   \n",
       "min        58.722225    48.024998    26.568422     2.786466     0.000700   \n",
       "25%       368.847214  1141.388794    38.286371     3.000036     0.096900   \n",
       "50%       645.000000  1186.111084    39.862242     3.021357     0.122100   \n",
       "75%      1098.472260  1233.541626    41.247956     3.050236     0.143100   \n",
       "max    567472.250000  1680.277832    62.972978     3.169298     1.499800   \n",
       "\n",
       "         Feature15    Feature16    Feature17    Feature18      Feature19  \\\n",
       "count  3202.000000  3201.000000  3202.000000  3362.000000    3362.000000   \n",
       "mean    177.615993     0.256455     0.325374   303.498199    1038.740703   \n",
       "std      27.270181     0.022156     0.005560    65.533392   20340.841285   \n",
       "min      55.760000     0.053119     0.209769     4.551250       0.458333   \n",
       "25%     165.200007     0.252126     0.322766   276.645828      53.525000   \n",
       "50%     175.800000     0.256889     0.325444   304.333344     111.694443   \n",
       "75%     187.450001     0.261532     0.328006   329.013886     231.819439   \n",
       "max     740.149990     0.968355     0.341348  2817.500000  555666.687500   \n",
       "\n",
       "         Feature20    Feature21    Feature22  Feature23    Feature24  \\\n",
       "count  3362.000000  3195.000000  3202.000000   8.000000  3202.000000   \n",
       "mean    569.128142    30.561336     3.231875   0.225650    95.265756   \n",
       "std      80.184771     2.594434     0.034437   0.233881    19.246906   \n",
       "min       7.652778    10.361442     3.058215   0.002000    60.245002   \n",
       "25%     531.201401    28.983149     3.209533   0.012875    87.326252   \n",
       "50%     570.555542    30.607371     3.226834   0.141350    94.039999   \n",
       "75%     607.499939    32.102097     3.252851   0.499800   101.250000   \n",
       "max    3332.500000    48.439015     4.006091   0.499800   566.000002   \n",
       "\n",
       "         Feature25         Label  \n",
       "count  3202.000000  26946.000000  \n",
       "mean     45.823957      0.256921  \n",
       "std       5.560062      0.436944  \n",
       "min       0.806450      0.000000  \n",
       "25%      42.486250      0.000000  \n",
       "50%      45.735000      0.000000  \n",
       "75%      49.188749      1.000000  \n",
       "max     123.299993      1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and drop bad data value.\n",
    "bad_idx = df[\"Feature07\"].idxmax()\n",
    "df.drop(bad_idx, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24287 entries, 1 to 26945\n",
      "Data columns (total 27 columns):\n",
      "Feature00    24287 non-null float64\n",
      "Feature01    24287 non-null float64\n",
      "Feature02    24287 non-null float64\n",
      "Feature03    24287 non-null float64\n",
      "Feature04    24287 non-null int8\n",
      "Feature05    24287 non-null float64\n",
      "Feature06    24287 non-null float64\n",
      "Feature07    24287 non-null float64\n",
      "Feature08    24287 non-null float64\n",
      "Feature09    24287 non-null float64\n",
      "Feature10    24287 non-null float64\n",
      "Feature11    24287 non-null float64\n",
      "Feature12    24287 non-null float64\n",
      "Feature13    24287 non-null float64\n",
      "Feature14    24287 non-null float64\n",
      "Feature15    24287 non-null float64\n",
      "Feature16    24287 non-null float64\n",
      "Feature17    24287 non-null float64\n",
      "Feature18    24287 non-null float64\n",
      "Feature19    24287 non-null float64\n",
      "Feature20    24287 non-null float64\n",
      "Feature21    24287 non-null float64\n",
      "Feature22    24287 non-null float64\n",
      "Feature23    24287 non-null float64\n",
      "Feature24    24287 non-null float64\n",
      "Feature25    24287 non-null float64\n",
      "Label        24287 non-null int64\n",
      "dtypes: float64(25), int64(1), int8(1)\n",
      "memory usage: 5.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature00</th>\n",
       "      <th>Feature01</th>\n",
       "      <th>Feature02</th>\n",
       "      <th>Feature03</th>\n",
       "      <th>Feature04</th>\n",
       "      <th>Feature05</th>\n",
       "      <th>Feature06</th>\n",
       "      <th>Feature07</th>\n",
       "      <th>Feature08</th>\n",
       "      <th>Feature09</th>\n",
       "      <th>Feature10</th>\n",
       "      <th>Feature11</th>\n",
       "      <th>Feature12</th>\n",
       "      <th>Feature13</th>\n",
       "      <th>Feature14</th>\n",
       "      <th>Feature15</th>\n",
       "      <th>Feature16</th>\n",
       "      <th>Feature17</th>\n",
       "      <th>Feature18</th>\n",
       "      <th>Feature19</th>\n",
       "      <th>Feature20</th>\n",
       "      <th>Feature21</th>\n",
       "      <th>Feature22</th>\n",
       "      <th>Feature23</th>\n",
       "      <th>Feature24</th>\n",
       "      <th>Feature25</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "      <td>24287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.958153</td>\n",
       "      <td>8.576644</td>\n",
       "      <td>16.358371</td>\n",
       "      <td>2.464311</td>\n",
       "      <td>17.781076</td>\n",
       "      <td>1.007915</td>\n",
       "      <td>10.436967</td>\n",
       "      <td>0.021969</td>\n",
       "      <td>0.035027</td>\n",
       "      <td>83.003963</td>\n",
       "      <td>265.872659</td>\n",
       "      <td>147.725565</td>\n",
       "      <td>4.752251</td>\n",
       "      <td>0.359796</td>\n",
       "      <td>0.015276</td>\n",
       "      <td>21.045749</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.038789</td>\n",
       "      <td>37.559413</td>\n",
       "      <td>136.847939</td>\n",
       "      <td>70.551632</td>\n",
       "      <td>3.654193</td>\n",
       "      <td>0.384958</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>11.315960</td>\n",
       "      <td>5.429204</td>\n",
       "      <td>0.280562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.148859</td>\n",
       "      <td>0.551513</td>\n",
       "      <td>31.204905</td>\n",
       "      <td>4.058040</td>\n",
       "      <td>8.878212</td>\n",
       "      <td>2.781532</td>\n",
       "      <td>33.727564</td>\n",
       "      <td>0.060166</td>\n",
       "      <td>0.095237</td>\n",
       "      <td>220.637828</td>\n",
       "      <td>9642.096597</td>\n",
       "      <td>391.942705</td>\n",
       "      <td>12.953545</td>\n",
       "      <td>0.978906</td>\n",
       "      <td>0.044788</td>\n",
       "      <td>57.996579</td>\n",
       "      <td>0.083585</td>\n",
       "      <td>0.105461</td>\n",
       "      <td>102.190210</td>\n",
       "      <td>7575.175968</td>\n",
       "      <td>188.922093</td>\n",
       "      <td>9.981984</td>\n",
       "      <td>1.046525</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>31.510017</td>\n",
       "      <td>14.874236</td>\n",
       "      <td>0.449283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.774274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.645206</td>\n",
       "      <td>8.175224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.112343</td>\n",
       "      <td>8.544922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.737906</td>\n",
       "      <td>8.931361</td>\n",
       "      <td>30.305754</td>\n",
       "      <td>8.402623</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.268906</td>\n",
       "      <td>11.309989</td>\n",
       "      <td>205.078095</td>\n",
       "      <td>10.906808</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>11.476805</td>\n",
       "      <td>422.399986</td>\n",
       "      <td>0.603558</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>1115.416626</td>\n",
       "      <td>567472.250000</td>\n",
       "      <td>1680.277832</td>\n",
       "      <td>62.972978</td>\n",
       "      <td>3.156810</td>\n",
       "      <td>1.499800</td>\n",
       "      <td>740.149990</td>\n",
       "      <td>0.968355</td>\n",
       "      <td>0.341348</td>\n",
       "      <td>2817.500000</td>\n",
       "      <td>555666.687500</td>\n",
       "      <td>3332.500000</td>\n",
       "      <td>48.439015</td>\n",
       "      <td>4.006091</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>566.000002</td>\n",
       "      <td>123.299993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Feature00     Feature01     Feature02     Feature03     Feature04  \\\n",
       "count  24287.000000  24287.000000  24287.000000  24287.000000  24287.000000   \n",
       "mean      33.958153      8.576644     16.358371      2.464311     17.781076   \n",
       "std       19.148859      0.551513     31.204905      4.058040      8.878212   \n",
       "min        0.000000      5.774274      0.000000      0.000000      0.000000   \n",
       "25%       21.645206      8.175224      0.000000      0.000000     10.000000   \n",
       "50%       28.112343      8.544922      0.000000      0.000000     22.000000   \n",
       "75%       40.737906      8.931361     30.305754      8.402623     27.000000   \n",
       "max      142.268906     11.309989    205.078095     10.906808     27.000000   \n",
       "\n",
       "          Feature05     Feature06     Feature07     Feature08     Feature09  \\\n",
       "count  24287.000000  24287.000000  24287.000000  24287.000000  24287.000000   \n",
       "mean       1.007915     10.436967      0.021969      0.035027     83.003963   \n",
       "std        2.781532     33.727564      0.060166      0.095237    220.637828   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       11.476805    422.399986      0.603558      0.309033   1115.416626   \n",
       "\n",
       "           Feature10     Feature11     Feature12     Feature13     Feature14  \\\n",
       "count   24287.000000  24287.000000  24287.000000  24287.000000  24287.000000   \n",
       "mean      265.872659    147.725565      4.752251      0.359796      0.015276   \n",
       "std      9642.096597    391.942705     12.953545      0.978906      0.044788   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max    567472.250000   1680.277832     62.972978      3.156810      1.499800   \n",
       "\n",
       "          Feature15     Feature16     Feature17     Feature18      Feature19  \\\n",
       "count  24287.000000  24287.000000  24287.000000  24287.000000   24287.000000   \n",
       "mean      21.045749      0.030603      0.038789     37.559413     136.847939   \n",
       "std       57.996579      0.083585      0.105461    102.190210    7575.175968   \n",
       "min        0.000000      0.000000      0.000000      0.000000       0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000       0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000       0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000       0.000000   \n",
       "max      740.149990      0.968355      0.341348   2817.500000  555666.687500   \n",
       "\n",
       "          Feature20     Feature21     Feature22     Feature23     Feature24  \\\n",
       "count  24287.000000  24287.000000  24287.000000  24287.000000  24287.000000   \n",
       "mean      70.551632      3.654193      0.384958      0.000062     11.315960   \n",
       "std      188.922093      9.981984      1.046525      0.005555     31.510017   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     3332.500000     48.439015      4.006091      0.499800    566.000002   \n",
       "\n",
       "          Feature25         Label  \n",
       "count  24287.000000  24287.000000  \n",
       "mean       5.429204      0.280562  \n",
       "std       14.874236      0.449283  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%        0.000000      0.000000  \n",
       "75%        0.000000      1.000000  \n",
       "max      123.299993      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature01 is a data collection issue, drop NaNs.\n",
    "df.dropna(subset=['Feature01'], inplace=True)\n",
    "\n",
    "# Features 0, 2, and 3 should have NaN's zeroed out. I think this\n",
    "# step is unnecessary since we fill the remaining columns with 0\n",
    "# afterwards.\n",
    "zero_cols = {'Feature00': 0, 'Feature02': 0, 'Feature03': 0}\n",
    "df.fillna(zero_cols, inplace=True)\n",
    "\n",
    "# Fill other NaN's. The assignment says to do this only after having\n",
    "# fixed features 00-03, but I'm not convinced it matters (besides\n",
    "# dropping rows with NaNs in Feature01).\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Divide into training, validation, and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels and data.\n",
    "x = df.drop('Label', axis=1).values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split into train and test.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Further split to get validation data.\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6) Normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler object.\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# Fit to the training data, then transform it.\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "# Transform testing and validation data.\n",
    "x_test = scaler.transform(x_test)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7) Classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Naive Bayes\n",
      "Confusion matrix:\n",
      "[[ 507 3035]\n",
      " [  11 1304]]\n",
      "F1 score for testing data: 0.4613\n",
      "Training accuracy: 0.3761\n",
      "Testing accuracy: 0.3729\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L2, C=0.01\n",
      "Confusion matrix:\n",
      "[[3483   59]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9365\n",
      "Training accuracy: 0.9673\n",
      "Testing accuracy: 0.9662\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L2, C=0.1\n",
      "Confusion matrix:\n",
      "[[3502   40]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9435\n",
      "Training accuracy: 0.9717\n",
      "Testing accuracy: 0.9701\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L2, C=1\n",
      "Confusion matrix:\n",
      "[[3511   31]\n",
      " [ 106 1209]]\n",
      "F1 score for testing data: 0.9464\n",
      "Training accuracy: 0.9732\n",
      "Testing accuracy: 0.9718\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L2, C=10\n",
      "Confusion matrix:\n",
      "[[3512   30]\n",
      " [ 106 1209]]\n",
      "F1 score for testing data: 0.9468\n",
      "Training accuracy: 0.9732\n",
      "Testing accuracy: 0.9720\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L1, C=0.01\n",
      "Confusion matrix:\n",
      "[[3485   57]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9373\n",
      "Training accuracy: 0.9675\n",
      "Testing accuracy: 0.9666\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L1, C=0.1\n",
      "Confusion matrix:\n",
      "[[3508   34]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9457\n",
      "Training accuracy: 0.9726\n",
      "Testing accuracy: 0.9714\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L1, C=1\n",
      "Confusion matrix:\n",
      "[[3512   30]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9472\n",
      "Training accuracy: 0.9733\n",
      "Testing accuracy: 0.9722\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Logistic Regression, L1, C=10\n",
      "Confusion matrix:\n",
      "[[3512   30]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9472\n",
      "Training accuracy: 0.9734\n",
      "Testing accuracy: 0.9722\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L2\n",
      "Confusion matrix:\n",
      "[[3503   39]\n",
      " [ 113 1202]]\n",
      "F1 score for testing data: 0.9405\n",
      "Training accuracy: 0.9704\n",
      "Testing accuracy: 0.9687\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L1, alpha=0.0001\n",
      "Confusion matrix:\n",
      "[[3520   22]\n",
      " [ 112 1203]]\n",
      "F1 score for testing data: 0.9472\n",
      "Training accuracy: 0.9736\n",
      "Testing accuracy: 0.9724\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L1, alpha=0.001\n",
      "Confusion matrix:\n",
      "[[3509   33]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9461\n",
      "Training accuracy: 0.9730\n",
      "Testing accuracy: 0.9716\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L1, alpha=0.01\n",
      "Confusion matrix:\n",
      "[[3482   60]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9362\n",
      "Training accuracy: 0.9672\n",
      "Testing accuracy: 0.9660\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L1, alpha=0.1\n",
      "Confusion matrix:\n",
      "[[3482   60]\n",
      " [ 105 1210]]\n",
      "F1 score for testing data: 0.9362\n",
      "Training accuracy: 0.9672\n",
      "Testing accuracy: 0.9660\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, MH\n",
      "Confusion matrix:\n",
      "[[3499   43]\n",
      " [ 113 1202]]\n",
      "F1 score for testing data: 0.9391\n",
      "Training accuracy: 0.9688\n",
      "Testing accuracy: 0.9679\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, SH\n",
      "Confusion matrix:\n",
      "[[3302  240]\n",
      " [ 133 1182]]\n",
      "F1 score for testing data: 0.8637\n",
      "Training accuracy: 0.9232\n",
      "Testing accuracy: 0.9232\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for storing results.\n",
    "f1 = []\n",
    "c_str = []\n",
    "\n",
    "# **NAIVE BAYES**\n",
    "# Initialize and train.\n",
    "nb_classifier = GaussianNB()\n",
    "# Train, predict, score.\n",
    "# noinspection PyTypeChecker\n",
    "cm_nb, f1_nb = train_predict(model=nb_classifier, x_train=x_train,\n",
    "                             y_train=y_train, x_test=x_val, y_test=y_val,\n",
    "                             c_str='Naive Bayes')\n",
    "f1.append(f1_nb)\n",
    "c_str.append('Naive Bayes')\n",
    "\n",
    "# **LOGISTIC REGRESSION**\n",
    "# L2 regularization:\n",
    "for c in [0.01, 0.1, 1, 10]:\n",
    "    s = 'Logistic Regression, L2, C={}'.format(c)\n",
    "    lr2_classifier = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                        penalty='l2', max_iter=1000,\n",
    "                                        C=c)\n",
    "    cm_lr2, f1_lr2 = train_predict(model=lr2_classifier, x_train=x_train,\n",
    "                                   y_train=y_train, x_test=x_val,\n",
    "                                   y_test=y_val,\n",
    "                                   c_str=s)\n",
    "    f1.append(f1_lr2)\n",
    "    c_str.append(s)\n",
    "\n",
    "# L1 regularization:\n",
    "for c in [0.01, 0.1, 1, 10]:\n",
    "    s = 'Logistic Regression, L1, C={}'.format(c)\n",
    "    lr1_classifier = LogisticRegression(random_state=0, solver='saga',\n",
    "                                        penalty='l1', max_iter=1000,\n",
    "                                        C=c)\n",
    "    cm_lr1, f1_lr1 = train_predict(model=lr1_classifier,\n",
    "                                   x_train=x_train,\n",
    "                                   y_train=y_train, x_test=x_val,\n",
    "                                   y_test=y_val,\n",
    "                                   c_str=s)\n",
    "    f1.append(f1_lr1)\n",
    "    c_str.append(s)\n",
    "\n",
    "# ** Stochastic Gradient Descent **\n",
    "# L2 regularization:\n",
    "sgd_classifier2 = SGDClassifier(max_iter=1000, tol=1e-4, penalty='l2')\n",
    "# noinspection PyTypeChecker\n",
    "cm_sgd2, f1_sgd2 = train_predict(model=sgd_classifier2, x_train=x_train,\n",
    "                                 y_train=y_train, x_test=x_val,\n",
    "                                 y_test=y_val,\n",
    "                                 c_str='Stochastic Gradient Descent, L2')\n",
    "f1.append(f1_sgd2)\n",
    "c_str.append('Stochastic Gradient Descent, L2')\n",
    "\n",
    "# L1 regularization:\n",
    "for a in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    s = 'Stochastic Gradient Descent, L1, alpha={}'.format(a)\n",
    "    sgd_classifier1 = SGDClassifier(max_iter=1000, tol=1e-4, penalty='l1',\n",
    "                                    alpha=a)\n",
    "    # noinspection PyTypeChecker\n",
    "    cm_sgd1, f1_sgd1 = \\\n",
    "        train_predict(model=sgd_classifier1,\n",
    "                      x_train=x_train,\n",
    "                      y_train=y_train, x_test=x_val,\n",
    "                      y_test=y_val,\n",
    "                      c_str=s)\n",
    "    f1.append(f1_sgd1)\n",
    "    c_str.append(s)\n",
    "\n",
    "# L2, modified_huber loss\n",
    "sgd_classifier_mh = SGDClassifier(max_iter=1000, tol=1e-4, penalty='l2',\n",
    "                                  loss='modified_huber')\n",
    "# noinspection PyTypeChecker\n",
    "cm_sgd_mh, f1_sgd_mh = \\\n",
    "    train_predict(model=sgd_classifier_mh,\n",
    "                  x_train=x_train,\n",
    "                  y_train=y_train, x_test=x_val,\n",
    "                  y_test=y_val,\n",
    "                  c_str='Stochastic Gradient Descent, MH')\n",
    "f1.append(f1_sgd_mh)\n",
    "c_str.append('Stochastic Gradient Descent, MH')\n",
    "\n",
    "# L2, squared_hinge loss\n",
    "sgd_classifier_sh = SGDClassifier(max_iter=1000, tol=1e-4, penalty='l2',\n",
    "                                  loss='squared_hinge')\n",
    "# noinspection PyTypeChecker\n",
    "cm_sgd_sh, f1_sgd_sh = \\\n",
    "    train_predict(model=sgd_classifier_sh,\n",
    "                  x_train=x_train,\n",
    "                  y_train=y_train, x_test=x_val,\n",
    "                  y_test=y_val,\n",
    "                  c_str='Stochastic Gradient Descent, SH')\n",
    "f1.append(f1_sgd_sh)\n",
    "c_str.append('Stochastic Gradient Descent, SH')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8) Test time.\n",
    "### Determine best model/hyperparemeters, run on test dataset, compute F1 score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.9472\n",
      "Corresponding model: Stochastic Gradient Descent, L1, alpha=0.0001\n",
      "********************************************************************************\n",
      "Stochastic Gradient Descent, L1, alpha=0.0001\n",
      "Confusion matrix:\n",
      "[[3469   22]\n",
      " [ 105 1262]]\n",
      "F1 score for testing data: 0.9521\n",
      "Training accuracy: 0.9722\n",
      "Testing accuracy: 0.9739\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print('Best F1 score: {:.4f}'.format(max(f1)))\n",
    "# noinspection PyTypeChecker\n",
    "print('Corresponding model: {}'.format(c_str[np.argmax(np.array(f1))]))\n",
    "\n",
    "sgd_classifier1 = SGDClassifier(max_iter=1000, tol=1e-4, penalty='l1',\n",
    "                                alpha=0.0001)\n",
    "# noinspection PyTypeChecker\n",
    "cm_sgd1, f1_sgd1 = \\\n",
    "    train_predict(model=sgd_classifier1,\n",
    "                  x_train=np.vstack((x_train, x_val)),\n",
    "                  y_train=np.hstack((y_train, y_val)),\n",
    "                  x_test=x_test, y_test=y_test,\n",
    "                  c_str='Stochastic Gradient Descent, L1, alpha=0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9) One more shot at Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 2 (0.470232)\n",
      "2. feature 3 (0.380666)\n",
      "3. feature 0 (0.073046)\n",
      "4. feature 1 (0.048608)\n",
      "5. feature 4 (0.009252)\n",
      "6. feature 6 (0.001922)\n",
      "7. feature 10 (0.001245)\n",
      "8. feature 12 (0.001090)\n",
      "9. feature 11 (0.001060)\n",
      "10. feature 22 (0.001055)\n",
      "11. feature 9 (0.000989)\n",
      "12. feature 13 (0.000943)\n",
      "13. feature 8 (0.000943)\n",
      "14. feature 18 (0.000904)\n",
      "15. feature 16 (0.000894)\n",
      "16. feature 25 (0.000882)\n",
      "17. feature 19 (0.000876)\n",
      "18. feature 17 (0.000833)\n",
      "19. feature 24 (0.000816)\n",
      "20. feature 20 (0.000726)\n",
      "21. feature 5 (0.000654)\n",
      "22. feature 14 (0.000636)\n",
      "23. feature 21 (0.000625)\n",
      "24. feature 15 (0.000596)\n",
      "25. feature 7 (0.000508)\n",
      "26. feature 23 (0.000000)\n",
      "********************************************************************************\n",
      "Naive Bayes, 2 Features\n",
      "Confusion matrix:\n",
      "[[3438   53]\n",
      " [ 103 1264]]\n",
      "F1 score for testing data: 0.9419\n",
      "Training accuracy: 0.9669\n",
      "Testing accuracy: 0.9679\n",
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create RandomForest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# print feature importances in descending order\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f],\n",
    "                                   importances[indices[f]]))\n",
    "\n",
    "# Extract top two features from our already scaled data.\n",
    "x_train_2 = x_train[:, indices[0:2]]\n",
    "x_val_2 = x_val[:, indices[0:2]]\n",
    "x_test_2 = x_test[:, indices[0:2]]\n",
    "\n",
    "# Perform Naive Bayes again.\n",
    "# Initialize and train.\n",
    "nb2 = GaussianNB()\n",
    "# Train, predict, and score. Use the full training + validation set\n",
    "# for training, and the testing dataset for testing.\n",
    "# noinspection PyTypeChecker\n",
    "cm_nb2, f1_nb2 = train_predict(model=nb2,\n",
    "                               x_train=np.vstack((x_train_2, x_val_2)),\n",
    "                               y_train=np.hstack((y_train, y_val)),\n",
    "                               x_test=x_test_2,\n",
    "                               y_test=y_test,\n",
    "                               c_str='Naive Bayes, 2 Features')\n",
    "\n",
    "# Plot the two most important features on a scatter.\n",
    "df_2 = pd.DataFrame(dict(x1=x_val_2[:, 0], x2=x_val_2[:, 1], label=y_val))\n",
    "colors = {1: 'blue', 0: 'red'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df_2.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x1', y='x2', marker='o', s=100,\n",
    "               alpha=0.2, label=key,\n",
    "               color=colors[key])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# Set min and max values and give it some padding\n",
    "x_min, x_max = x_val_2[:, 0].min() - .5, x_val_2[:, 0].max() + .5\n",
    "y_min, y_max = x_val_2[:, 1].min() - .5, x_val_2[:, 1].max() + .5\n",
    "h = 0.1\n",
    "# Generate a grid of points with distance h between them\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "# Predict the function value for the whole grid\n",
    "z = nb2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "z = z.reshape(xx.shape)\n",
    "# Plot the contour and test examples\n",
    "# noinspection PyUnresolvedReferences\n",
    "plt.contourf(xx, yy, z, cmap=plt.cm.Spectral)\n",
    "# noinspection PyUnresolvedReferences\n",
    "plt.scatter(x_val_2[:, 0], x_val_2[:, 1], c=y_val, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obesrvations on plot above.\n",
    "Note that while it appears the red region in the contour plot is bad, there are in fact many 0-labeled points stacked up on each other in that bottom-left red dot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
